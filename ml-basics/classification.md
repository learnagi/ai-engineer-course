---
title: "åˆ†ç±»ç®—æ³•è¯¦è§£"
slug: "classification"
sequence: 5
description: "ä»é€»è¾‘å›å½’åˆ°é›†æˆå­¦ä¹ ï¼ŒæŒæ¡ä¸»æµåˆ†ç±»ç®—æ³•"
is_published: true
estimated_minutes: 90
language: "zh-CN"
---

![åˆ†ç±»ç®—æ³•](https://z1.zve.cn/tutorial/classification/classification-overview.png)
*åˆ†ç±»ç®—æ³•æ˜¯æœºå™¨å­¦ä¹ ä¸­æœ€å¸¸ç”¨çš„æŠ€æœ¯ä¹‹ä¸€ï¼Œè®©æˆ‘ä»¬ç³»ç»Ÿåœ°å­¦ä¹ å®ƒä»¬*

# åˆ†ç±»ç®—æ³•è¯¦è§£

## æœ¬èŠ‚æ¦‚è¦

é€šè¿‡æœ¬èŠ‚å­¦ä¹ ï¼Œä½ å°†ï¼š
- ç†è§£å„ç§åˆ†ç±»ç®—æ³•çš„åŸç†å’Œæ•°å­¦åŸºç¡€
- æŒæ¡ä¸åŒåˆ†ç±»å™¨çš„å®ç°æ–¹æ³•å’Œè°ƒä¼˜æŠ€å·§
- å­¦ä¼šé€‰æ‹©åˆé€‚çš„åˆ†ç±»ç®—æ³•è§£å†³å®é™…é—®é¢˜
- æŒæ¡å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†çš„ç­–ç•¥
- èƒ½å¤Ÿè¯„ä¼°å’Œä¼˜åŒ–åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½

ğŸ’¡ é‡ç‚¹å†…å®¹ï¼š
- ä»çº¿æ€§å›å½’åˆ°é€»è¾‘å›å½’çš„æ¨å¯¼è¿‡ç¨‹
- å†³ç­–æ ‘çš„ç”Ÿé•¿å’Œå‰ªæç­–ç•¥
- æ”¯æŒå‘é‡æœºçš„æ ¸æŠ€å·§åº”ç”¨
- é›†æˆå­¦ä¹ æå‡æ¨¡å‹æ€§èƒ½

## å­¦ä¹ ç›®æ ‡
å®Œæˆæœ¬èŠ‚åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ç†è§£ä¸»è¦åˆ†ç±»ç®—æ³•çš„åŸç†å’Œç‰¹ç‚¹
- å®ç°å’Œä½¿ç”¨å„ç§åˆ†ç±»å™¨
- é€‰æ‹©åˆé€‚çš„åˆ†ç±»ç®—æ³•
- è¯„ä¼°åˆ†ç±»æ¨¡å‹çš„æ€§èƒ½
- å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†é—®é¢˜

## å…ˆä¿®çŸ¥è¯†
å­¦ä¹ æœ¬èŠ‚å†…å®¹éœ€è¦ï¼š
- Pythonç¼–ç¨‹åŸºç¡€
- æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ
- æ•°å­¦å’Œç»Ÿè®¡å­¦åŸºç¡€
- æ•°æ®é¢„å¤„ç†çŸ¥è¯†

## é€»è¾‘å›å½’

### ä»çº¿æ€§å›å½’åˆ°é€»è¾‘å›å½’
çº¿æ€§å›å½’é¢„æµ‹è¿ç»­å€¼ï¼Œè€Œåˆ†ç±»é—®é¢˜éœ€è¦é¢„æµ‹ç¦»æ•£ç±»åˆ«ã€‚é€»è¾‘å›å½’é€šè¿‡åœ¨çº¿æ€§å›å½’çš„åŸºç¡€ä¸Šæ·»åŠ sigmoidå‡½æ•°ï¼Œå°†è¾“å‡ºæ˜ å°„åˆ°[0,1]åŒºé—´ï¼Œä»è€Œå®ç°äºŒåˆ†ç±»ã€‚

![Sigmoidå‡½æ•°](./images/classification/sigmoid-function.png)
*Sigmoidå‡½æ•°å°†ä»»æ„å®æ•°æ˜ å°„åˆ°(0,1)åŒºé—´ï¼Œæ˜¯é€»è¾‘å›å½’çš„æ ¸å¿ƒç»„ä»¶*

### æ•°å­¦åŸç†

#### 1. æ¨¡å‹å½¢å¼
å¯¹äºè¾“å…¥ç‰¹å¾xï¼Œé€»è¾‘å›å½’æ¨¡å‹çš„é¢„æµ‹æ¦‚ç‡ä¸ºï¼š

```
P(y=1|x) = Ïƒ(w^T x + b)
```

å…¶ä¸­ï¼š
- Ïƒ(z) = 1/(1+e^(-z)) æ˜¯sigmoidå‡½æ•°
- wæ˜¯æƒé‡å‘é‡
- bæ˜¯åç½®é¡¹

#### 2. æŸå¤±å‡½æ•°
é€»è¾‘å›å½’ä½¿ç”¨å¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼ˆLog Likelihood Lossï¼‰ï¼š

```
L(w) = -âˆ‘[y_i log(p_i) + (1-y_i)log(1-p_i)]
```

è¿™ä¸ªæŸå¤±å‡½æ•°çš„ç‰¹ç‚¹ï¼š
- å½“é¢„æµ‹æ­£ç¡®æ—¶ï¼ŒæŸå¤±æ¥è¿‘0
- å½“é¢„æµ‹é”™è¯¯æ—¶ï¼ŒæŸå¤±å¾ˆå¤§
- æ˜¯ä¸€ä¸ªå‡¸å‡½æ•°ï¼Œå¯ä»¥ä¿è¯æ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£

#### 3. ä¼˜åŒ–æ–¹æ³•
ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ³•ä¼˜åŒ–ï¼š
1. è®¡ç®—æŸå¤±å‡½æ•°å¯¹wçš„æ¢¯åº¦
2. æ²¿ç€æ¢¯åº¦çš„åæ–¹å‘æ›´æ–°å‚æ•°
3. é‡å¤ç›´åˆ°æ”¶æ•›

### å†³ç­–è¾¹ç•Œå¯è§†åŒ–

ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„äºŒç»´æ•°æ®åˆ†ç±»ç¤ºä¾‹ï¼Œå±•ç¤ºäº†é€»è¾‘å›å½’çš„å†³ç­–è¾¹ç•Œï¼š

![é€»è¾‘å›å½’å†³ç­–è¾¹ç•Œ](./images/classification/logistic-decision-boundary.png)
*é€»è¾‘å›å½’åœ¨äºŒç»´ç‰¹å¾ç©ºé—´ä¸­åˆ’åˆ†å‡ºä¸€æ¡ç›´çº¿ä½œä¸ºå†³ç­–è¾¹ç•Œ*

### å®ç°ç¤ºä¾‹

```python
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.datasets import make_classification

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
X, y = make_classification(
    n_samples=1000,    # æ ·æœ¬æ•°
    n_features=20,     # ç‰¹å¾æ•°
    n_classes=2,       # ç±»åˆ«æ•°
    random_state=42
)

# åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
model = LogisticRegression(
    penalty='l2',      # L2æ­£åˆ™åŒ–
    C=1.0,            # æ­£åˆ™åŒ–å¼ºåº¦çš„å€’æ•°
    solver='lbfgs'     # ä¼˜åŒ–ç®—æ³•
)
model.fit(X, y)

# é¢„æµ‹æ¦‚ç‡å’Œç±»åˆ«
y_prob = model.predict_proba(X)    # é¢„æµ‹æ¦‚ç‡
y_pred = model.predict(X)          # é¢„æµ‹ç±»åˆ«
```

### ä¼˜ç¼ºç‚¹åˆ†æ

ä¼˜ç‚¹ï¼š
1. æ¨¡å‹ç®€å•ï¼Œè®¡ç®—æ•ˆç‡é«˜
2. å¯è§£é‡Šæ€§å¼º
3. ä¸æ˜“è¿‡æ‹Ÿåˆ
4. å¯ä»¥è¾“å‡ºæ¦‚ç‡
5. å¯ä»¥ä½¿ç”¨æ­£åˆ™åŒ–

ç¼ºç‚¹ï¼š
1. å‡è®¾ç‰¹å¾ä¹‹é—´ç›¸äº’ç‹¬ç«‹
2. åªèƒ½å¤„ç†çº¿æ€§å¯åˆ†çš„é—®é¢˜
3. å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ
4. è¦æ±‚ç‰¹å¾å’Œç›®æ ‡å˜é‡ä¹‹é—´æ˜¯Sigmoidå…³ç³»

### åº”ç”¨åœºæ™¯

é€»è¾‘å›å½’é€‚ç”¨äºï¼š
1. éœ€è¦æ¦‚ç‡è¾“å‡ºçš„åœºæ™¯
   - åƒåœ¾é‚®ä»¶æ£€æµ‹
   - ç–¾ç—…é£é™©é¢„æµ‹
   - ä¿¡ç”¨è¯„åˆ†

2. éœ€è¦å¯è§£é‡Šæ€§çš„åœºæ™¯
   - åŒ»ç–—è¯Šæ–­
   - é‡‘èé£æ§
   - è¥é”€é¢„æµ‹

3. ç‰¹å¾ä¹‹é—´ç›¸å¯¹ç‹¬ç«‹çš„åœºæ™¯
   - æ–‡æœ¬åˆ†ç±»
   - ç®€å•å›¾åƒåˆ†ç±»
   - ç”¨æˆ·è¡Œä¸ºé¢„æµ‹

### å®è·µæŠ€å·§

1. **ç‰¹å¾å·¥ç¨‹**
   - å¤„ç†ç¼ºå¤±å€¼
   - ç‰¹å¾æ ‡å‡†åŒ–
   - å¤„ç†ç±»åˆ«ç‰¹å¾
   - ç‰¹å¾é€‰æ‹©

2. **å‚æ•°è°ƒä¼˜**
   - æ­£åˆ™åŒ–ç±»å‹ï¼ˆL1/L2ï¼‰
   - æ­£åˆ™åŒ–å¼ºåº¦ï¼ˆCå€¼ï¼‰
   - ä¼˜åŒ–å™¨é€‰æ‹©
   - è¿­ä»£æ¬¡æ•°

3. **æ¨¡å‹è¯„ä¼°**
   - æ··æ·†çŸ©é˜µ
   - ROCæ›²çº¿
   - PRæ›²çº¿
   - äº¤å‰éªŒè¯

## Kè¿‘é‚»ç®—æ³•(KNN)

### åŸç†ä¸å®ç°
KNNæ˜¯ä¸€ç§åŸºäºå®ä¾‹çš„å­¦ä¹ ç®—æ³•ï¼Œé€šè¿‡è®¡ç®—æ ·æœ¬é—´è·ç¦»è¿›è¡Œåˆ†ç±»ã€‚

```python
from sklearn.neighbors import KNeighborsClassifier

# åˆ›å»ºKNNåˆ†ç±»å™¨
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X, y)

# é¢„æµ‹
y_pred_knn = knn.predict(X)
```

### é€‰æ‹©æœ€ä¼˜Kå€¼
```python
from sklearn.model_selection import cross_val_score

def find_best_k(X, y, k_range):
    """
    é€šè¿‡äº¤å‰éªŒè¯é€‰æ‹©æœ€ä¼˜çš„Kå€¼
    """
    k_scores = []
    for k in k_range:
        knn = KNeighborsClassifier(n_neighbors=k)
        scores = cross_val_score(knn, X, y, cv=5)
        k_scores.append(scores.mean())
    
    return k_scores

# æµ‹è¯•ä¸åŒçš„Kå€¼
k_range = range(1, 31)
k_scores = find_best_k(X, y, k_range)

# å¯è§†åŒ–ç»“æœ
plt.plot(k_range, k_scores)
plt.xlabel('Kå€¼')
plt.ylabel('äº¤å‰éªŒè¯å¾—åˆ†')
plt.title('ä¸åŒKå€¼çš„æ¨¡å‹æ€§èƒ½')
plt.show()
```

## æ”¯æŒå‘é‡æœº(SVM)

### çº¿æ€§SVM
```python
from sklearn.svm import SVC

# åˆ›å»ºçº¿æ€§SVM
svm_linear = SVC(kernel='linear')
svm_linear.fit(X, y)

# é¢„æµ‹
y_pred_svm = svm_linear.predict(X)
```

### æ ¸æ–¹æ³•
```python
# RBFæ ¸SVM
svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale')
svm_rbf.fit(X, y)

# å¤šé¡¹å¼æ ¸SVM
svm_poly = SVC(kernel='poly', degree=3)
svm_poly.fit(X, y)
```

## å†³ç­–æ ‘

### å†³ç­–æ ‘åˆ†ç±»å™¨
```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree

# åˆ›å»ºå†³ç­–æ ‘
dt = DecisionTreeClassifier(max_depth=5)
dt.fit(X, y)

# å¯è§†åŒ–å†³ç­–æ ‘
plt.figure(figsize=(20,10))
plot_tree(dt, filled=True, feature_names=[f'feature_{i}' for i in range(X.shape[1])])
plt.show()
```

### ç‰¹å¾é‡è¦æ€§
```python
def plot_feature_importance(model, feature_names):
    """
    å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
    """
    importances = model.feature_importances_
    indices = np.argsort(importances)[::-1]
    
    plt.figure(figsize=(10,6))
    plt.title("ç‰¹å¾é‡è¦æ€§")
    plt.bar(range(X.shape[1]), importances[indices])
    plt.xticks(range(X.shape[1]), [feature_names[i] for i in indices], rotation=45)
    plt.show()
```

## é›†æˆæ–¹æ³•

### éšæœºæ£®æ—
```python
from sklearn.ensemble import RandomForestClassifier

# åˆ›å»ºéšæœºæ£®æ—
rf = RandomForestClassifier(n_estimators=100, max_depth=5)
rf.fit(X, y)

# é¢„æµ‹
y_pred_rf = rf.predict(X)
```

### æ¢¯åº¦æå‡
```python
from sklearn.ensemble import GradientBoostingClassifier

# åˆ›å»ºæ¢¯åº¦æå‡åˆ†ç±»å™¨
gb = GradientBoostingClassifier(n_estimators=100)
gb.fit(X, y)

# é¢„æµ‹
y_pred_gb = gb.predict(X)
```

## æ¨¡å‹è¯„ä¼°

### è¯„ä¼°æŒ‡æ ‡
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix, classification_report

def evaluate_classifier(y_true, y_pred):
    """
    è¯„ä¼°åˆ†ç±»å™¨æ€§èƒ½
    """
    print("å‡†ç¡®ç‡:", accuracy_score(y_true, y_pred))
    print("ç²¾ç¡®ç‡:", precision_score(y_true, y_pred))
    print("å¬å›ç‡:", recall_score(y_true, y_pred))
    print("F1åˆ†æ•°:", f1_score(y_true, y_pred))
    print("\næ··æ·†çŸ©é˜µ:\n", confusion_matrix(y_true, y_pred))
    print("\nåˆ†ç±»æŠ¥å‘Š:\n", classification_report(y_true, y_pred))
```

### ROCæ›²çº¿
```python
from sklearn.metrics import roc_curve, auc

def plot_roc_curve(y_true, y_prob):
    """
    ç»˜åˆ¶ROCæ›²çº¿
    """
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)
    
    plt.figure(figsize=(10,6))
    plt.plot(fpr, tpr, color='darkorange', lw=2,
             label=f'ROCæ›²çº¿ (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('å‡æ­£ç‡')
    plt.ylabel('çœŸæ­£ç‡')
    plt.title('æ¥æ”¶è€…æ“ä½œç‰¹å¾æ›²çº¿')
    plt.legend(loc="lower right")
    plt.show()
```

## å¤„ç†ä¸å¹³è¡¡æ•°æ®

### é‡é‡‡æ ·æ–¹æ³•
```python
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline

# SMOTEè¿‡é‡‡æ ·
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X, y)

# éšæœºæ¬ é‡‡æ ·
rus = RandomUnderSampler()
X_resampled, y_resampled = rus.fit_resample(X, y)

# ç»“åˆè¿‡é‡‡æ ·å’Œæ¬ é‡‡æ ·
pipeline = Pipeline([
    ('smote', SMOTE()),
    ('rus', RandomUnderSampler())
])
X_resampled, y_resampled = pipeline.fit_resample(X, y)
```

### ç±»åˆ«æƒé‡
```python
# ä½¿ç”¨ç±»åˆ«æƒé‡
weighted_model = LogisticRegression(class_weight='balanced')
weighted_model.fit(X, y)
```

## å®æˆ˜é¡¹ç›®ï¼šä¿¡ç”¨å¡æ¬ºè¯ˆæ£€æµ‹

### æ•°æ®å‡†å¤‡
```python
# åŠ è½½æ•°æ®
from sklearn.datasets import make_classification
X, y = make_classification(n_samples=10000, n_features=20,
                         n_classes=2, weights=[0.99, 0.01],
                         random_state=42)

# æ•°æ®åˆ†å‰²
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42)
```

### æ¨¡å‹è®­ç»ƒä¸è¯„ä¼°
```python
# åˆ›å»ºæ¨¡å‹ç®¡é“
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('classifier', RandomForestClassifier(class_weight='balanced'))
])

# è®­ç»ƒæ¨¡å‹
pipeline.fit(X_train, y_train)

# é¢„æµ‹å’Œè¯„ä¼°
y_pred = pipeline.predict(X_test)
evaluate_classifier(y_test, y_pred)
```

## ç»ƒä¹ ä¸ä½œä¸š
1. åŸºç¡€ç»ƒä¹ ï¼š
   - å®ç°é€»è¾‘å›å½’çš„æ¢¯åº¦ä¸‹é™
   - ä½¿ç”¨ä¸åŒæ ¸å‡½æ•°çš„SVM
   - æ„å»ºå’Œå¯è§†åŒ–å†³ç­–æ ‘

2. è¿›é˜¶ç»ƒä¹ ï¼š
   - å®ç°äº¤å‰éªŒè¯å’Œç½‘æ ¼æœç´¢
   - å¤„ç†ä¸å¹³è¡¡æ•°æ®é›†
   - æ¯”è¾ƒä¸åŒåˆ†ç±»å™¨çš„æ€§èƒ½

3. é¡¹ç›®å®è·µï¼š
   - é€‰æ‹©ä¸€ä¸ªçœŸå®æ•°æ®é›†è¿›è¡Œåˆ†ç±»
   - å®ç°å®Œæ•´çš„åˆ†ç±»æµç¨‹
   - å°è¯•ä¸åŒçš„æ¨¡å‹ä¼˜åŒ–æ–¹æ³•

## å¸¸è§é—®é¢˜
Q1: å¦‚ä½•é€‰æ‹©åˆé€‚çš„åˆ†ç±»ç®—æ³•ï¼Ÿ
A1: éœ€è¦è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š
- æ•°æ®è§„æ¨¡å’Œç»´åº¦
- ç‰¹å¾çš„çº¿æ€§å¯åˆ†æ€§
- è®¡ç®—èµ„æºé™åˆ¶
- æ¨¡å‹å¯è§£é‡Šæ€§éœ€æ±‚
- é¢„æµ‹é€Ÿåº¦è¦æ±‚

Q2: å¦‚ä½•å¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Ÿ
A2: å¯ä»¥é‡‡ç”¨ä»¥ä¸‹æ–¹æ³•ï¼š
- å¢åŠ è®­ç»ƒæ•°æ®
- ä½¿ç”¨æ­£åˆ™åŒ–
- å‡å°‘æ¨¡å‹å¤æ‚åº¦
- ä½¿ç”¨é›†æˆæ–¹æ³•
- ç‰¹å¾é€‰æ‹©

## æ‰©å±•é˜…è¯»
- [scikit-learnåˆ†ç±»ç®—æ³•æŒ‡å—](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)
- [ä¸å¹³è¡¡å­¦ä¹ ](https://imbalanced-learn.org/stable/)
- [é›†æˆå­¦ä¹ æ–¹æ³•](https://scikit-learn.org/stable/modules/ensemble.html)

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- æ·±åº¦å­¦ä¹ åˆ†ç±»
- åºåˆ—åˆ†ç±»
- å¤šæ ‡ç­¾åˆ†ç±»
- åŠç›‘ç£å­¦ä¹ 
