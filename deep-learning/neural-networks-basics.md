---
title: "ç¥ç»ç½‘ç»œåŸºç¡€"
slug: "neural-networks"
sequence: 9
description: "æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µå’ŒåŸºç¡€ç»„ä»¶ï¼ŒåŒ…æ‹¬ç¥ç»ç½‘ç»œç»“æ„ã€æ¿€æ´»å‡½æ•°ã€åå‘ä¼ æ’­ç­‰åŸºç¡€çŸ¥è¯†"
is_published: true
estimated_minutes: 120
language: "zh-CN"
---

# ç¥ç»ç½‘ç»œåŸºç¡€

## è¯¾ç¨‹ä»‹ç»
æœ¬æ¨¡å—æ·±å…¥è®²è§£ç¥ç»ç½‘ç»œçš„åŸºç¡€æ¦‚å¿µå’Œæ ¸å¿ƒç»„ä»¶ï¼Œé€šè¿‡å®ç°ä¸€ä¸ªç®€å•çš„ç¥ç»ç½‘ç»œæ¥ç†è§£æ·±åº¦å­¦ä¹ çš„åŸºæœ¬åŸç†ã€‚

## å­¦ä¹ ç›®æ ‡
å®Œæˆæœ¬æ¨¡å—å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ç†è§£ç¥ç»ç½‘ç»œçš„åŸºæœ¬ç»“æ„
- æŒæ¡å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
- å®ç°å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°
- æ„å»ºç®€å•çš„ç¥ç»ç½‘ç»œ

## 1. ç¥ç»ç½‘ç»œç»“æ„

### 1.1 ç¥ç»å…ƒæ¨¡å‹
```python
# ğŸ§  å®æˆ˜æ¡ˆä¾‹ï¼šç¥ç»å…ƒå®ç°
import numpy as np

class Neuron:
    """å•ä¸ªç¥ç»å…ƒçš„å®ç°"""
    def __init__(self, n_inputs):
        # éšæœºåˆå§‹åŒ–æƒé‡
        self.weights = np.random.randn(n_inputs) * 0.01
        self.bias = 0
        
        # å­˜å‚¨ä¸­é—´å€¼ç”¨äºåå‘ä¼ æ’­
        self.x = None
        self.output = None
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        self.x = x
        # è®¡ç®—åŠ æƒå’Œ
        z = np.dot(x, self.weights) + self.bias
        # ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°
        self.output = np.maximum(0, z)
        return self.output
    
    def backward(self, grad_output):
        """åå‘ä¼ æ’­"""
        # ReLUçš„æ¢¯åº¦
        grad_z = grad_output * (self.output > 0)
        
        # è®¡ç®—æ¢¯åº¦
        grad_weights = np.outer(self.x, grad_z)
        grad_bias = np.sum(grad_z)
        grad_x = np.dot(grad_z, self.weights.T)
        
        return grad_x, grad_weights, grad_bias

# æµ‹è¯•ç¥ç»å…ƒ
def test_neuron():
    """æµ‹è¯•ç¥ç»å…ƒçš„å‰å‘å’Œåå‘ä¼ æ’­"""
    # åˆ›å»ºç¥ç»å…ƒ
    neuron = Neuron(n_inputs=3)
    
    # æµ‹è¯•æ•°æ®
    x = np.array([1.0, 2.0, 3.0])
    
    # å‰å‘ä¼ æ’­
    output = neuron.forward(x)
    print(f"è¾“å‡º: {output}")
    
    # åå‘ä¼ æ’­
    grad_output = 1.0
    grad_x, grad_w, grad_b = neuron.backward(grad_output)
    print(f"è¾“å…¥æ¢¯åº¦: {grad_x}")
    print(f"æƒé‡æ¢¯åº¦: {grad_w}")
    print(f"åç½®æ¢¯åº¦: {grad_b}")
```

### 1.2 å±‚çš„å®ç°
```python
# ğŸ”„ å®æˆ˜æ¡ˆä¾‹ï¼šå…¨è¿æ¥å±‚å®ç°
class FCLayer:
    """å…¨è¿æ¥å±‚å®ç°"""
    def __init__(self, n_inputs, n_units):
        self.weights = np.random.randn(n_inputs, n_units) * 0.01
        self.bias = np.zeros(n_units)
        
        self.x = None
        self.output = None
        
        # ä¼˜åŒ–å™¨çŠ¶æ€
        self.momentum_w = np.zeros_like(self.weights)
        self.momentum_b = np.zeros_like(self.bias)
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        self.x = x
        self.output = np.dot(x, self.weights) + self.bias
        return self.output
    
    def backward(self, grad_output):
        """åå‘ä¼ æ’­"""
        # è®¡ç®—æ¢¯åº¦
        grad_weights = np.dot(self.x.T, grad_output)
        grad_bias = np.sum(grad_output, axis=0)
        grad_x = np.dot(grad_output, self.weights.T)
        
        return grad_x, grad_weights, grad_bias
    
    def update(self, grad_weights, grad_bias, learning_rate=0.01, momentum=0.9):
        """ä½¿ç”¨åŠ¨é‡æ›´æ–°å‚æ•°"""
        # æ›´æ–°åŠ¨é‡
        self.momentum_w = momentum * self.momentum_w - learning_rate * grad_weights
        self.momentum_b = momentum * self.momentum_b - learning_rate * grad_bias
        
        # æ›´æ–°å‚æ•°
        self.weights += self.momentum_w
        self.bias += self.momentum_b
```

## 2. æ¿€æ´»å‡½æ•°

### 2.1 å¸¸ç”¨æ¿€æ´»å‡½æ•°
```python
# âš¡ï¸ å®æˆ˜æ¡ˆä¾‹ï¼šæ¿€æ´»å‡½æ•°å®ç°
class Activation:
    """æ¿€æ´»å‡½æ•°é›†åˆ"""
    @staticmethod
    def relu(x):
        """ReLUæ¿€æ´»å‡½æ•°"""
        return np.maximum(0, x)
    
    @staticmethod
    def relu_derivative(x):
        """ReLUå¯¼æ•°"""
        return (x > 0).astype(float)
    
    @staticmethod
    def sigmoid(x):
        """Sigmoidæ¿€æ´»å‡½æ•°"""
        return 1 / (1 + np.exp(-x))
    
    @staticmethod
    def sigmoid_derivative(x):
        """Sigmoidå¯¼æ•°"""
        s = Activation.sigmoid(x)
        return s * (1 - s)
    
    @staticmethod
    def tanh(x):
        """Tanhæ¿€æ´»å‡½æ•°"""
        return np.tanh(x)
    
    @staticmethod
    def tanh_derivative(x):
        """Tanhå¯¼æ•°"""
        return 1 - np.tanh(x)**2

# å¯è§†åŒ–æ¿€æ´»å‡½æ•°
def plot_activations():
    """å¯è§†åŒ–ä¸åŒçš„æ¿€æ´»å‡½æ•°"""
    x = np.linspace(-5, 5, 100)
    
    plt.figure(figsize=(15, 5))
    
    # ReLU
    plt.subplot(1, 3, 1)
    plt.plot(x, Activation.relu(x), label='ReLU')
    plt.plot(x, Activation.relu_derivative(x), label='å¯¼æ•°')
    plt.title('ReLU')
    plt.legend()
    
    # Sigmoid
    plt.subplot(1, 3, 2)
    plt.plot(x, Activation.sigmoid(x), label='Sigmoid')
    plt.plot(x, Activation.sigmoid_derivative(x), label='å¯¼æ•°')
    plt.title('Sigmoid')
    plt.legend()
    
    # Tanh
    plt.subplot(1, 3, 3)
    plt.plot(x, Activation.tanh(x), label='Tanh')
    plt.plot(x, Activation.tanh_derivative(x), label='å¯¼æ•°')
    plt.title('Tanh')
    plt.legend()
    
    return plt.gcf()
```

## 3. å‰å‘ä¼ æ’­ä¸åå‘ä¼ æ’­

### 3.1 ç®€å•ç¥ç»ç½‘ç»œå®ç°
```python
# ğŸ§® å®æˆ˜æ¡ˆä¾‹ï¼šä¸¤å±‚ç¥ç»ç½‘ç»œ
class SimpleNN:
    """ç®€å•çš„ä¸¤å±‚ç¥ç»ç½‘ç»œ"""
    def __init__(self, input_size, hidden_size, output_size):
        self.hidden = FCLayer(input_size, hidden_size)
        self.output = FCLayer(hidden_size, output_size)
        self.activation = Activation()
    
    def forward(self, x):
        """å‰å‘ä¼ æ’­"""
        # ç¬¬ä¸€å±‚
        hidden_output = self.hidden.forward(x)
        hidden_activated = self.activation.relu(hidden_output)
        
        # ç¬¬äºŒå±‚
        output = self.output.forward(hidden_activated)
        return self.activation.sigmoid(output)
    
    def backward(self, x, y, output):
        """åå‘ä¼ æ’­"""
        batch_size = x.shape[0]
        
        # è¾“å‡ºå±‚æ¢¯åº¦
        grad_output = (output - y) / batch_size
        grad_output *= self.activation.sigmoid_derivative(self.output.output)
        grad_h, grad_w2, grad_b2 = self.output.backward(grad_output)
        
        # éšè—å±‚æ¢¯åº¦
        grad_h *= self.activation.relu_derivative(self.hidden.output)
        grad_x, grad_w1, grad_b1 = self.hidden.backward(grad_h)
        
        return (grad_w1, grad_b1), (grad_w2, grad_b2)
    
    def train_step(self, x, y, learning_rate=0.01):
        """è®­ç»ƒä¸€æ­¥"""
        # å‰å‘ä¼ æ’­
        output = self.forward(x)
        
        # è®¡ç®—æŸå¤±
        loss = -np.mean(y * np.log(output + 1e-8) + 
                       (1 - y) * np.log(1 - output + 1e-8))
        
        # åå‘ä¼ æ’­
        (grad_w1, grad_b1), (grad_w2, grad_b2) = self.backward(x, y, output)
        
        # æ›´æ–°å‚æ•°
        self.hidden.update(grad_w1, grad_b1, learning_rate)
        self.output.update(grad_w2, grad_b2, learning_rate)
        
        return loss
```

### 3.2 è®­ç»ƒè¿‡ç¨‹
```python
# ğŸ“ˆ å®æˆ˜æ¡ˆä¾‹ï¼šè®­ç»ƒç¥ç»ç½‘ç»œ
def train_network():
    """è®­ç»ƒç¥ç»ç½‘ç»œç¤ºä¾‹"""
    # ç”Ÿæˆæ•°æ®
    np.random.seed(42)
    X = np.random.randn(1000, 2)
    y = (X[:, 0] + X[:, 1] > 0).astype(float).reshape(-1, 1)
    
    # åˆ›å»ºç½‘ç»œ
    network = SimpleNN(input_size=2, hidden_size=4, output_size=1)
    
    # è®­ç»ƒå‚æ•°
    epochs = 100
    batch_size = 32
    learning_rate = 0.1
    losses = []
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        epoch_losses = []
        
        # æ‰¹é‡è®­ç»ƒ
        for i in range(0, len(X), batch_size):
            batch_X = X[i:i+batch_size]
            batch_y = y[i:i+batch_size]
            
            loss = network.train_step(batch_X, batch_y, learning_rate)
            epoch_losses.append(loss)
        
        # è®°å½•å¹³å‡æŸå¤±
        avg_loss = np.mean(epoch_losses)
        losses.append(avg_loss)
        
        if epoch % 10 == 0:
            print(f"Epoch {epoch}, Loss: {avg_loss:.4f}")
    
    return network, losses

# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
def plot_training(losses):
    """å¯è§†åŒ–è®­ç»ƒæŸå¤±"""
    plt.figure(figsize=(10, 5))
    plt.plot(losses)
    plt.title('è®­ç»ƒæŸå¤±')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    return plt.gcf()
```

## å®æˆ˜é¡¹ç›®ï¼šæ‰‹å†™æ•°å­—è¯†åˆ«

### é¡¹ç›®æè¿°
ä½¿ç”¨ç®€å•çš„ç¥ç»ç½‘ç»œå®ç°MNISTæ‰‹å†™æ•°å­—è¯†åˆ«ã€‚

### é¡¹ç›®ä»£ç æ¡†æ¶
```python
class DigitClassifier:
    def __init__(self):
        # 784 -> 128 -> 10
        self.network = SimpleNN(784, 128, 10)
        self.history = []
    
    def preprocess_data(self, X):
        """é¢„å¤„ç†æ•°æ®"""
        # å½’ä¸€åŒ–
        X = X.astype(float) / 255.0
        # å±•å¹³å›¾åƒ
        X = X.reshape(X.shape[0], -1)
        return X
    
    def one_hot_encode(self, y):
        """ç‹¬çƒ­ç¼–ç """
        n_classes = 10
        n_samples = len(y)
        one_hot = np.zeros((n_samples, n_classes))
        one_hot[np.arange(n_samples), y] = 1
        return one_hot
    
    def train(self, X_train, y_train, X_val, y_val, 
             epochs=10, batch_size=32, learning_rate=0.01):
        """è®­ç»ƒæ¨¡å‹"""
        # é¢„å¤„ç†æ•°æ®
        X_train = self.preprocess_data(X_train)
        X_val = self.preprocess_data(X_val)
        
        # ç‹¬çƒ­ç¼–ç æ ‡ç­¾
        y_train = self.one_hot_encode(y_train)
        y_val = self.one_hot_encode(y_val)
        
        for epoch in range(epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            train_losses = []
            for i in range(0, len(X_train), batch_size):
                batch_X = X_train[i:i+batch_size]
                batch_y = y_train[i:i+batch_size]
                
                loss = self.network.train_step(
                    batch_X, batch_y, learning_rate
                )
                train_losses.append(loss)
            
            # éªŒè¯
            val_output = self.network.forward(X_val)
            val_loss = -np.mean(y_val * np.log(val_output + 1e-8) + 
                              (1 - y_val) * np.log(1 - val_output + 1e-8))
            
            # è®¡ç®—å‡†ç¡®ç‡
            val_pred = np.argmax(val_output, axis=1)
            val_true = np.argmax(y_val, axis=1)
            accuracy = np.mean(val_pred == val_true)
            
            # è®°å½•å†å²
            self.history.append({
                'train_loss': np.mean(train_losses),
                'val_loss': val_loss,
                'val_accuracy': accuracy
            })
            
            print(f"Epoch {epoch+1}/{epochs}")
            print(f"train_loss: {np.mean(train_losses):.4f}")
            print(f"val_loss: {val_loss:.4f}")
            print(f"val_accuracy: {accuracy:.4f}")
    
    def predict(self, X):
        """é¢„æµ‹"""
        X = self.preprocess_data(X)
        output = self.network.forward(X)
        return np.argmax(output, axis=1)
    
    def visualize_predictions(self, X, y_true, n_samples=5):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        # éšæœºé€‰æ‹©æ ·æœ¬
        indices = np.random.choice(len(X), n_samples, replace=False)
        X_sample = X[indices]
        y_true = y_true[indices]
        
        # é¢„æµ‹
        y_pred = self.predict(X_sample)
        
        # å¯è§†åŒ–
        plt.figure(figsize=(15, 3))
        for i in range(n_samples):
            plt.subplot(1, n_samples, i+1)
            plt.imshow(X_sample[i], cmap='gray')
            plt.title(f'True: {y_true[i]}\nPred: {y_pred[i]}')
            plt.axis('off')
        
        return plt.gcf()
```

## ç»ƒä¹ ä¸ä½œä¸š
1. å®ç°ä¸åŒçš„ä¼˜åŒ–å™¨ï¼ˆSGDã€Adamï¼‰
2. æ·»åŠ æ‰¹é‡å½’ä¸€åŒ–å±‚
3. å°è¯•ä¸åŒçš„ç½‘ç»œæ¶æ„

## æ‰©å±•é˜…è¯»
- [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- [Deep Learning Book](https://www.deeplearningbook.org/)
- [CS231nè¯¾ç¨‹ç¬”è®°](https://cs231n.github.io/)

## å°æµ‹éªŒ
1. ä¸åŒæ¿€æ´»å‡½æ•°çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
2. åå‘ä¼ æ’­ç®—æ³•çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ
3. å¦‚ä½•å¤„ç†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Ÿ

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- å·ç§¯ç¥ç»ç½‘ç»œ
- å¾ªç¯ç¥ç»ç½‘ç»œ
- æ·±åº¦å­¦ä¹ æ¡†æ¶ä½¿ç”¨

## å¸¸è§é—®é¢˜è§£ç­”
Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç½‘ç»œæ¶æ„ï¼Ÿ
A: æ ¹æ®é—®é¢˜ç±»å‹ã€æ•°æ®è§„æ¨¡å’Œè®¡ç®—èµ„æºé€‰æ‹©ã€‚å›¾åƒä»»åŠ¡é€šå¸¸ç”¨CNNï¼Œåºåˆ—ä»»åŠ¡ç”¨RNNï¼Œç®€å•ä»»åŠ¡å¯ä»¥ç”¨å¤šå±‚æ„ŸçŸ¥æœºã€‚

Q: å¦‚ä½•è°ƒæ•´å­¦ä¹ ç‡ï¼Ÿ
A: å¯ä»¥ä»è¾ƒå°çš„å€¼å¼€å§‹ï¼Œè§‚å¯Ÿè®­ç»ƒæ›²çº¿ã€‚å¦‚æœæ”¶æ•›å¤ªæ…¢å¯ä»¥å¢å¤§ï¼Œå¦‚æœä¸ç¨³å®šåˆ™å‡å°ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
