---
title: "æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ"
slug: "ml-basics"
sequence: 7
description: "æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µå’ŒåŸºç¡€ç†è®ºï¼ŒåŒ…æ‹¬å­¦ä¹ èŒƒå¼ã€æ¨¡å‹è¯„ä¼°ã€ç‰¹å¾å·¥ç¨‹ç­‰åŸºç¡€çŸ¥è¯†"
is_published: true
estimated_minutes: 90
language: "zh-CN"
---

# æœºå™¨å­¦ä¹ åŸºç¡€æ¦‚å¿µ

## è¯¾ç¨‹ä»‹ç»
æœ¬æ¨¡å—ä»‹ç»æœºå™¨å­¦ä¹ çš„æ ¸å¿ƒæ¦‚å¿µå’ŒåŸºç¡€ç†è®ºï¼Œå¸®åŠ©ä½ å»ºç«‹å®Œæ•´çš„æœºå™¨å­¦ä¹ çŸ¥è¯†ä½“ç³»ã€‚é€šè¿‡å®ä¾‹è®²è§£å’Œä»£ç å®è·µï¼Œæ·±å…¥ç†è§£æœºå™¨å­¦ä¹ çš„æœ¬è´¨ã€‚

## å­¦ä¹ ç›®æ ‡
å®Œæˆæœ¬æ¨¡å—å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ç†è§£æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µå’Œç±»å‹
- æŒæ¡æ¨¡å‹è¯„ä¼°çš„æ–¹æ³•å’ŒæŒ‡æ ‡
- ç†Ÿç»ƒè¿›è¡Œç‰¹å¾å·¥ç¨‹
- åº”å¯¹è¿‡æ‹Ÿåˆå’Œæ¬ æ‹Ÿåˆé—®é¢˜

## 1. æœºå™¨å­¦ä¹ åŸºç¡€

### 1.1 å­¦ä¹ èŒƒå¼
```python
# ğŸ¯ å®æˆ˜æ¡ˆä¾‹ï¼šä¸åŒç±»å‹çš„å­¦ä¹ ä»»åŠ¡
import numpy as np
from sklearn.model_selection import train_test_split

# ç›‘ç£å­¦ä¹ ç¤ºä¾‹
def supervised_learning_demo():
    """åˆ†ç±»ä»»åŠ¡ç¤ºä¾‹"""
    # ç”Ÿæˆæ•°æ®
    X = np.random.randn(1000, 20)  # ç‰¹å¾
    y = (X[:, 0] + X[:, 1] > 0).astype(int)  # æ ‡ç­¾
    
    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42
    )
    return X_train, X_test, y_train, y_test

# æ— ç›‘ç£å­¦ä¹ ç¤ºä¾‹
def unsupervised_learning_demo():
    """èšç±»ä»»åŠ¡ç¤ºä¾‹"""
    from sklearn.cluster import KMeans
    
    # ç”Ÿæˆæ•°æ®
    X = np.random.randn(1000, 2)
    
    # èšç±»
    kmeans = KMeans(n_clusters=3)
    clusters = kmeans.fit_predict(X)
    return X, clusters

# å¼ºåŒ–å­¦ä¹ ç¤ºä¾‹
class SimpleEnvironment:
    """ç®€å•çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ"""
    def __init__(self):
        self.state = 0
        
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›å¥–åŠ±"""
        if action == 1:  # å‘å³ç§»åŠ¨
            self.state += 1
        else:  # å‘å·¦ç§»åŠ¨
            self.state -= 1
        
        # è®¡ç®—å¥–åŠ±
        reward = -abs(self.state)  # è·ç¦»åŸç‚¹è¶Šè¿œå¥–åŠ±è¶Šå°
        done = abs(self.state) > 5  # åˆ°è¾¾è¾¹ç•Œåˆ™ç»“æŸ
        
        return self.state, reward, done
```

### 1.2 æ¨¡å‹è¯„ä¼°
```python
# ğŸ“Š å®æˆ˜æ¡ˆä¾‹ï¼šæ¨¡å‹è¯„ä¼°æ–¹æ³•
from sklearn.metrics import accuracy_score, precision_score, recall_score
from sklearn.model_selection import cross_val_score

def evaluate_model(model, X, y):
    """ç»¼åˆè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    # äº¤å‰éªŒè¯
    cv_scores = cross_val_score(model, X, y, cv=5)
    print(f"äº¤å‰éªŒè¯åˆ†æ•°: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})")
    
    # å­¦ä¹ æ›²çº¿
    def plot_learning_curve(model, X, y):
        from sklearn.model_selection import learning_curve
        
        train_sizes, train_scores, valid_scores = learning_curve(
            model, X, y, train_sizes=np.linspace(0.1, 1.0, 10),
            cv=5, n_jobs=-1
        )
        
        plt.figure(figsize=(10, 6))
        plt.plot(train_sizes, train_scores.mean(axis=1), label='è®­ç»ƒé›†')
        plt.plot(train_sizes, valid_scores.mean(axis=1), label='éªŒè¯é›†')
        plt.xlabel('è®­ç»ƒæ ·æœ¬æ•°')
        plt.ylabel('å¾—åˆ†')
        plt.title('å­¦ä¹ æ›²çº¿')
        plt.legend()
        return plt.gcf()
    
    # æ··æ·†çŸ©é˜µ
    def plot_confusion_matrix(y_true, y_pred):
        from sklearn.metrics import confusion_matrix
        import seaborn as sns
        
        cm = confusion_matrix(y_true, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.xlabel('é¢„æµ‹æ ‡ç­¾')
        plt.ylabel('çœŸå®æ ‡ç­¾')
        plt.title('æ··æ·†çŸ©é˜µ')
        return plt.gcf()
    
    return {
        'cv_scores': cv_scores,
        'learning_curve': plot_learning_curve(model, X, y),
        'confusion_matrix': plot_confusion_matrix(y, model.predict(X))
    }
```

## 2. ç‰¹å¾å·¥ç¨‹

### 2.1 ç‰¹å¾é¢„å¤„ç†
```python
# ğŸ”§ å®æˆ˜æ¡ˆä¾‹ï¼šç‰¹å¾é¢„å¤„ç†Pipeline
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.impute import SimpleImputer

def create_preprocessing_pipeline():
    """åˆ›å»ºç‰¹å¾é¢„å¤„ç†Pipeline"""
    numeric_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='mean')),
        ('scaler', StandardScaler())
    ])
    
    categorical_pipeline = Pipeline([
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(drop='first', sparse=False))
    ])
    
    # ç»„åˆPipeline
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numeric_pipeline, numeric_features),
            ('cat', categorical_pipeline, categorical_features)
        ])
    
    return preprocessor

def analyze_features(X):
    """åˆ†æç‰¹å¾è´¨é‡"""
    # ç¼ºå¤±å€¼åˆ†æ
    missing = pd.DataFrame({
        'missing_count': X.isnull().sum(),
        'missing_ratio': X.isnull().sum() / len(X)
    })
    
    # ç‰¹å¾åˆ†å¸ƒåˆ†æ
    distributions = {}
    for col in X.select_dtypes(include=[np.number]).columns:
        distributions[col] = {
            'mean': X[col].mean(),
            'std': X[col].std(),
            'skew': X[col].skew()
        }
    
    # ç‰¹å¾ç›¸å…³æ€§åˆ†æ
    correlation = X.corr()
    
    return {
        'missing_analysis': missing,
        'distributions': distributions,
        'correlation': correlation
    }
```

### 2.2 ç‰¹å¾é€‰æ‹©
```python
# ğŸ¯ å®æˆ˜æ¡ˆä¾‹ï¼šç‰¹å¾é€‰æ‹©æ–¹æ³•
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.ensemble import RandomForestClassifier

def select_features(X, y):
    """ç»¼åˆç‰¹å¾é€‰æ‹©"""
    # 1. æ–¹å·®åˆ†æ
    from sklearn.feature_selection import VarianceThreshold
    selector = VarianceThreshold(threshold=0.01)
    X_var = selector.fit_transform(X)
    
    # 2. å•å˜é‡ç‰¹å¾é€‰æ‹©
    selector = SelectKBest(score_func=f_classif, k=10)
    X_uni = selector.fit_transform(X, y)
    
    # 3. åŸºäºæ¨¡å‹çš„ç‰¹å¾é€‰æ‹©
    rf = RandomForestClassifier(n_estimators=100)
    rf.fit(X, y)
    importances = pd.DataFrame({
        'feature': X.columns,
        'importance': rf.feature_importances_
    }).sort_values('importance', ascending=False)
    
    return {
        'variance_selection': X_var,
        'univariate_selection': X_uni,
        'feature_importance': importances
    }
```

## 3. æ¨¡å‹ä¼˜åŒ–

### 3.1 è¿‡æ‹Ÿåˆä¸æ¬ æ‹Ÿåˆ
```python
# ğŸ¯ å®æˆ˜æ¡ˆä¾‹ï¼šæ¨¡å‹å¤æ‚åº¦åˆ†æ
def analyze_model_complexity():
    """åˆ†ææ¨¡å‹å¤æ‚åº¦å¯¹æ€§èƒ½çš„å½±å“"""
    from sklearn.linear_model import LogisticRegression
    from sklearn.preprocessing import PolynomialFeatures
    
    # ç”Ÿæˆæ•°æ®
    X = np.random.randn(1000, 1)
    y = (X[:, 0]**2 + np.random.randn(1000) * 0.1 > 0).astype(int)
    
    # æµ‹è¯•ä¸åŒå¤æ‚åº¦
    degrees = [1, 2, 3, 4, 5]
    train_scores = []
    test_scores = []
    
    for degree in degrees:
        # åˆ›å»ºå¤šé¡¹å¼ç‰¹å¾
        poly = PolynomialFeatures(degree=degree)
        X_poly = poly.fit_transform(X)
        
        # è®­ç»ƒæ¨¡å‹
        model = LogisticRegression()
        model.fit(X_poly, y)
        
        # è®°å½•å¾—åˆ†
        train_scores.append(model.score(X_poly, y))
        test_scores.append(np.mean(cross_val_score(model, X_poly, y, cv=5)))
    
    return degrees, train_scores, test_scores
```

### 3.2 æ­£åˆ™åŒ–æ–¹æ³•
```python
# ğŸ› ï¸ å®æˆ˜æ¡ˆä¾‹ï¼šæ­£åˆ™åŒ–æ•ˆæœå¯¹æ¯”
def compare_regularization():
    """æ¯”è¾ƒä¸åŒæ­£åˆ™åŒ–æ–¹æ³•çš„æ•ˆæœ"""
    from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
    
    # ç”Ÿæˆæ•°æ®
    X = np.random.randn(100, 20)
    y = X[:, 0] * 2 + X[:, 1] * 0.5 + np.random.randn(100) * 0.1
    
    # ä¸åŒæ¨¡å‹
    models = {
        'Linear': LinearRegression(),
        'Ridge': Ridge(alpha=1.0),
        'Lasso': Lasso(alpha=1.0),
        'ElasticNet': ElasticNet(alpha=1.0, l1_ratio=0.5)
    }
    
    # æ¯”è¾ƒç»“æœ
    results = {}
    for name, model in models.items():
        model.fit(X, y)
        results[name] = {
            'coefficients': model.coef_,
            'score': model.score(X, y)
        }
    
    return results
```

## å®æˆ˜é¡¹ç›®ï¼šæˆ¿ä»·é¢„æµ‹ç³»ç»Ÿ

### é¡¹ç›®æè¿°
æ„å»ºä¸€ä¸ªå®Œæ•´çš„æˆ¿ä»·é¢„æµ‹ç³»ç»Ÿï¼Œç»¼åˆè¿ç”¨ç‰¹å¾å·¥ç¨‹ã€æ¨¡å‹é€‰æ‹©å’Œè¯„ä¼°æ–¹æ³•ã€‚

### é¡¹ç›®ä»£ç æ¡†æ¶
```python
class HousePricePredictor:
    def __init__(self):
        self.preprocessor = None
        self.model = None
        self.feature_importance = None
    
    def preprocess_data(self, X):
        """æ•°æ®é¢„å¤„ç†"""
        # åˆ›å»ºé¢„å¤„ç†Pipeline
        numeric_features = X.select_dtypes(include=[np.number]).columns
        categorical_features = X.select_dtypes(exclude=[np.number]).columns
        
        self.preprocessor = create_preprocessing_pipeline(
            numeric_features, categorical_features
        )
        
        return self.preprocessor.fit_transform(X)
    
    def select_features(self, X, y):
        """ç‰¹å¾é€‰æ‹©"""
        feature_selector = SelectKBest(f_regression, k=20)
        X_selected = feature_selector.fit_transform(X, y)
        
        # è®°å½•ç‰¹å¾é‡è¦æ€§
        self.feature_importance = pd.DataFrame({
            'feature': X.columns,
            'importance': feature_selector.scores_
        }).sort_values('importance', ascending=False)
        
        return X_selected
    
    def train_model(self, X, y):
        """è®­ç»ƒæ¨¡å‹"""
        # åˆ›å»ºæ¨¡å‹Pipeline
        self.model = Pipeline([
            ('preprocessor', self.preprocessor),
            ('regressor', RandomForestRegressor(n_estimators=100))
        ])
        
        # è®­ç»ƒæ¨¡å‹
        self.model.fit(X, y)
        
        # è¯„ä¼°æ€§èƒ½
        scores = cross_val_score(self.model, X, y, cv=5)
        return np.mean(scores), np.std(scores)
    
    def predict(self, X):
        """é¢„æµ‹æˆ¿ä»·"""
        return self.model.predict(X)
    
    def explain_prediction(self, X):
        """è§£é‡Šé¢„æµ‹ç»“æœ"""
        import shap
        explainer = shap.TreeExplainer(self.model)
        shap_values = explainer.shap_values(X)
        
        return shap_values
```

## ç»ƒä¹ ä¸ä½œä¸š
1. å®ç°å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹Pipeline
2. æ¯”è¾ƒä¸åŒæ­£åˆ™åŒ–æ–¹æ³•çš„æ•ˆæœ
3. æ„å»ºæ¨¡å‹è¯„ä¼°æŠ¥å‘Šç³»ç»Ÿ

## æ‰©å±•é˜…è¯»
- [Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)
- [Feature Engineering for Machine Learning](https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/)
- [scikit-learnæ–‡æ¡£](https://scikit-learn.org/stable/user_guide.html)

## å°æµ‹éªŒ
1. æœºå™¨å­¦ä¹ çš„ä¸‰ç§ä¸»è¦ç±»å‹æ˜¯ä»€ä¹ˆï¼Ÿ
2. å¦‚ä½•å¤„ç†è¿‡æ‹Ÿåˆé—®é¢˜ï¼Ÿ
3. ç‰¹å¾é€‰æ‹©çš„ä¸»è¦æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- æ·±åº¦å­¦ä¹ åŸºç¡€
- æ¨¡å‹éƒ¨ç½²
- AutoMLæŠ€æœ¯

## å¸¸è§é—®é¢˜è§£ç­”
Q: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼Ÿ
A: æ ¹æ®æ•°æ®ç±»å‹ã€é—®é¢˜æ€§è´¨ã€æ ·æœ¬é‡å’Œè®¡ç®—èµ„æºç­‰å› ç´ ç»¼åˆè€ƒè™‘ã€‚åˆ†ç±»é—®é¢˜å¯ä»¥ä»ç®€å•çš„é€»è¾‘å›å½’å¼€å§‹ï¼Œå›å½’é—®é¢˜å¯ä»¥ä»çº¿æ€§å›å½’å¼€å§‹ã€‚

Q: ç‰¹å¾å·¥ç¨‹ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ
A: ç‰¹å¾å·¥ç¨‹ç›´æ¥å½±å“æ¨¡å‹æ€§èƒ½ï¼Œå¥½çš„ç‰¹å¾å¯ä»¥ç®€åŒ–æ¨¡å‹ç»“æ„ï¼Œæé«˜æ¨¡å‹å¯è§£é‡Šæ€§ï¼Œæ”¹å–„æ³›åŒ–èƒ½åŠ›ã€‚
