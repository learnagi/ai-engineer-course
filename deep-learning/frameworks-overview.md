---
title: "æ·±åº¦å­¦ä¹ æ¡†æ¶å®è·µ"
slug: "deep-learning-frameworks"
sequence: 10
description: "ä¸»æµæ·±åº¦å­¦ä¹ æ¡†æ¶PyTorchçš„ä½¿ç”¨ï¼ŒåŒ…æ‹¬æ¨¡å‹æ„å»ºã€è®­ç»ƒã€ä¼˜åŒ–å’Œéƒ¨ç½²"
is_published: true
estimated_minutes: 120
language: "zh-CN"
---

# æ·±åº¦å­¦ä¹ æ¡†æ¶å®è·µ

## è¯¾ç¨‹ä»‹ç»
æœ¬æ¨¡å—ä»‹ç»PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶çš„ä½¿ç”¨ï¼Œé€šè¿‡å®é™…æ¡ˆä¾‹å­¦ä¹ å¦‚ä½•æ„å»ºã€è®­ç»ƒå’Œéƒ¨ç½²æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

## å­¦ä¹ ç›®æ ‡
å®Œæˆæœ¬æ¨¡å—å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ä½¿ç”¨PyTorchæ„å»ºç¥ç»ç½‘ç»œ
- å®ç°æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°
- æŒæ¡æ•°æ®åŠ è½½å’Œé¢„å¤„ç†
- è¿›è¡Œæ¨¡å‹éƒ¨ç½²å’Œä¼˜åŒ–

## 1. PyTorchåŸºç¡€

### 1.1 å¼ é‡æ“ä½œ
```python
# ğŸ”¥ å®æˆ˜æ¡ˆä¾‹ï¼šPyTorchå¼ é‡æ“ä½œ
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader

def tensor_operations():
    """PyTorchå¼ é‡åŸºæœ¬æ“ä½œç¤ºä¾‹"""
    # åˆ›å»ºå¼ é‡
    x = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)
    y = torch.randn(2, 2)
    
    # åŸºæœ¬è¿ç®—
    print("åŠ æ³•:", x + y)
    print("çŸ©é˜µä¹˜æ³•:", torch.mm(x, y))
    print("å…ƒç´ ä¹˜æ³•:", x * y)
    
    # GPUæ”¯æŒ
    if torch.cuda.is_available():
        x_gpu = x.cuda()
        y_gpu = y.cuda()
        print("GPUå¼ é‡:", x_gpu)
    
    # æ¢¯åº¦è®¡ç®—
    x.requires_grad_(True)
    z = torch.sum(x ** 2)
    z.backward()
    print("æ¢¯åº¦:", x.grad)

# è‡ªå®šä¹‰æ•°æ®é›†
class CustomDataset(Dataset):
    """è‡ªå®šä¹‰æ•°æ®é›†ç¤ºä¾‹"""
    def __init__(self, X, y):
        self.X = torch.FloatTensor(X)
        self.y = torch.FloatTensor(y)
    
    def __len__(self):
        return len(self.X)
    
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]
```

### 1.2 ç¥ç»ç½‘ç»œæ¨¡å—
```python
# ğŸ§  å®æˆ˜æ¡ˆä¾‹ï¼šPyTorchç¥ç»ç½‘ç»œ
class SimpleNet(nn.Module):
    """ç®€å•ç¥ç»ç½‘ç»œç¤ºä¾‹"""
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        self.layer1 = nn.Linear(input_size, hidden_size)
        self.layer2 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(0.2)
    
    def forward(self, x):
        x = self.layer1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.layer2(x)
        return x

# è®­ç»ƒå‡½æ•°
def train_model(model, train_loader, criterion, optimizer, device):
    """æ¨¡å‹è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    running_loss = 0.0
    
    for inputs, targets in train_loader:
        inputs, targets = inputs.to(device), targets.to(device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        running_loss += loss.item()
    
    return running_loss / len(train_loader)

# è¯„ä¼°å‡½æ•°
def evaluate_model(model, val_loader, criterion, device):
    """æ¨¡å‹è¯„ä¼°"""
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for inputs, targets in val_loader:
            inputs, targets = inputs.to(device), targets.to(device)
            
            outputs = model(inputs)
            loss = criterion(outputs, targets)
            
            running_loss += loss.item()
            
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()
    
    accuracy = 100. * correct / total
    avg_loss = running_loss / len(val_loader)
    
    return avg_loss, accuracy
```

## 2. é«˜çº§ç‰¹æ€§

### 2.1 è‡ªå®šä¹‰å±‚å’ŒæŸå¤±å‡½æ•°
```python
# ğŸ› ï¸ å®æˆ˜æ¡ˆä¾‹ï¼šè‡ªå®šä¹‰ç»„ä»¶
class FocalLoss(nn.Module):
    """Focal Losså®ç°"""
    def __init__(self, alpha=1, gamma=2):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, inputs, targets):
        ce_loss = nn.functional.cross_entropy(inputs, targets, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss
        return focal_loss.mean()

class ResidualBlock(nn.Module):
    """æ®‹å·®å—å®ç°"""
    def __init__(self, in_channels):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(in_channels)
        self.conv2 = nn.Conv2d(in_channels, in_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(in_channels)
    
    def forward(self, x):
        residual = x
        out = nn.functional.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual
        return nn.functional.relu(out)
```

### 2.2 æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
```python
# ğŸ’¾ å®æˆ˜æ¡ˆä¾‹ï¼šæ¨¡å‹ä¿å­˜å’ŒåŠ è½½
def save_checkpoint(model, optimizer, epoch, filename):
    """ä¿å­˜æ£€æŸ¥ç‚¹"""
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
    }
    torch.save(checkpoint, filename)

def load_checkpoint(model, optimizer, filename):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    checkpoint = torch.load(filename)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return checkpoint['epoch']
```

## 3. æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–

### 3.1 è®­ç»ƒæµç¨‹
```python
# ğŸ“ˆ å®æˆ˜æ¡ˆä¾‹ï¼šå®Œæ•´è®­ç»ƒæµç¨‹
class Trainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    def __init__(self, model, criterion, optimizer, device):
        self.model = model
        self.criterion = criterion
        self.optimizer = optimizer
        self.device = device
        self.history = {'train_loss': [], 'val_loss': [], 'val_acc': []}
    
    def train_epoch(self, train_loader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        epoch_loss = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            
            # åå‘ä¼ æ’­
            loss.backward()
            self.optimizer.step()
            
            epoch_loss += loss.item()
            
            # æ‰“å°è¿›åº¦
            if batch_idx % 100 == 0:
                print(f'Train Batch: {batch_idx}/{len(train_loader)} '
                      f'Loss: {loss.item():.6f}')
        
        return epoch_loss / len(train_loader)
    
    def validate(self, val_loader):
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        val_loss = 0
        correct = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                val_loss += self.criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
        
        val_loss /= len(val_loader)
        accuracy = 100. * correct / len(val_loader.dataset)
        
        return val_loss, accuracy
    
    def train(self, train_loader, val_loader, epochs, 
             checkpoint_path='checkpoint.pt'):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        best_val_loss = float('inf')
        
        for epoch in range(epochs):
            print(f'\nEpoch: {epoch+1}/{epochs}')
            
            # è®­ç»ƒå’ŒéªŒè¯
            train_loss = self.train_epoch(train_loader)
            val_loss, val_acc = self.validate(val_loader)
            
            # æ›´æ–°å†å²è®°å½•
            self.history['train_loss'].append(train_loss)
            self.history['val_loss'].append(val_loss)
            self.history['val_acc'].append(val_acc)
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                save_checkpoint(self.model, self.optimizer, 
                              epoch, checkpoint_path)
            
            print(f'Train Loss: {train_loss:.4f}')
            print(f'Val Loss: {val_loss:.4f}')
            print(f'Val Accuracy: {val_acc:.2f}%')
    
    def plot_history(self):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        plt.figure(figsize=(12, 4))
        
        # æŸå¤±æ›²çº¿
        plt.subplot(1, 2, 1)
        plt.plot(self.history['train_loss'], label='train')
        plt.plot(self.history['val_loss'], label='val')
        plt.title('Loss History')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        
        # å‡†ç¡®ç‡æ›²çº¿
        plt.subplot(1, 2, 2)
        plt.plot(self.history['val_acc'])
        plt.title('Validation Accuracy')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy (%)')
        
        return plt.gcf()
```

## å®æˆ˜é¡¹ç›®ï¼šå›¾åƒåˆ†ç±»æ¨¡å‹

### é¡¹ç›®æè¿°
ä½¿ç”¨PyTorchæ„å»ºå’Œè®­ç»ƒä¸€ä¸ªå›¾åƒåˆ†ç±»æ¨¡å‹ï¼ŒåŒ…å«å®Œæ•´çš„è®­ç»ƒæµç¨‹å’Œæ¨¡å‹ä¼˜åŒ–ã€‚

### é¡¹ç›®ä»£ç æ¡†æ¶
```python
class ImageClassifier:
    def __init__(self, num_classes):
        # ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹
        self.model = models.resnet18(pretrained=True)
        # ä¿®æ”¹æœ€åä¸€å±‚
        self.model.fc = nn.Linear(512, num_classes)
        
        self.transform = transforms.Compose([
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406],
                              std=[0.229, 0.224, 0.225])
        ])
    
    def train(self, train_dir, val_dir, epochs=10, batch_size=32, 
             learning_rate=0.001):
        """è®­ç»ƒæ¨¡å‹"""
        # æ•°æ®åŠ è½½
        train_dataset = ImageFolder(train_dir, transform=self.transform)
        val_dataset = ImageFolder(val_dir, transform=self.transform)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size,
                                shuffle=True, num_workers=4)
        val_loader = DataLoader(val_dataset, batch_size=batch_size,
                              shuffle=False, num_workers=4)
        
        # è®¾ç½®è®¾å¤‡
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model = self.model.to(device)
        
        # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(self.model, criterion, optimizer, device)
        
        # è®­ç»ƒæ¨¡å‹
        trainer.train(train_loader, val_loader, epochs)
        
        return trainer.history
    
    def predict(self, image_path):
        """é¢„æµ‹å•å¼ å›¾ç‰‡"""
        # åŠ è½½å’Œé¢„å¤„ç†å›¾ç‰‡
        image = Image.open(image_path)
        image = self.transform(image).unsqueeze(0)
        
        # é¢„æµ‹
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(image)
            _, predicted = outputs.max(1)
        
        return predicted.item()
    
    def visualize_predictions(self, image_paths, class_names):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        plt.figure(figsize=(15, 3))
        for i, path in enumerate(image_paths):
            # åŠ è½½å’Œé¢„æµ‹
            pred = self.predict(path)
            
            # æ˜¾ç¤ºå›¾ç‰‡
            image = Image.open(path)
            plt.subplot(1, len(image_paths), i+1)
            plt.imshow(image)
            plt.title(f'Pred: {class_names[pred]}')
            plt.axis('off')
        
        return plt.gcf()
```

## ç»ƒä¹ ä¸ä½œä¸š
1. å®ç°ä¸åŒçš„ä¼˜åŒ–ç­–ç•¥ï¼ˆå­¦ä¹ ç‡è°ƒåº¦ã€æ—©åœç­‰ï¼‰
2. æ·»åŠ æ•°æ®å¢å¼ºæ–¹æ³•
3. å°è¯•ä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹

## æ‰©å±•é˜…è¯»
- [PyTorchæ–‡æ¡£](https://pytorch.org/docs/stable/index.html)
- [PyTorchæ•™ç¨‹](https://pytorch.org/tutorials/)
- [æ·±åº¦å­¦ä¹ å®æˆ˜](https://d2l.ai/)

## å°æµ‹éªŒ
1. PyTorchä¸­å¼ é‡å’ŒNumPyæ•°ç»„çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
2. å¦‚ä½•å¤„ç†GPUå†…å­˜ä¸è¶³çš„é—®é¢˜ï¼Ÿ
3. ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†ï¼Ÿ

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- é«˜çº§æ¨¡å‹æ¶æ„
- æ¨¡å‹éƒ¨ç½²å’ŒæœåŠ¡
- åˆ†å¸ƒå¼è®­ç»ƒ

## å¸¸è§é—®é¢˜è§£ç­”
Q: å¦‚ä½•é€‰æ‹©æ‰¹é‡å¤§å°ï¼Ÿ
A: æ‰¹é‡å¤§å°éœ€è¦å¹³è¡¡è®­ç»ƒé€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨ã€‚é€šå¸¸ä»16æˆ–32å¼€å§‹ï¼Œæ ¹æ®GPUå†…å­˜å’Œæ¨¡å‹æ€§èƒ½è°ƒæ•´ã€‚

Q: å¦‚ä½•å¤„ç†è¿‡æ‹Ÿåˆï¼Ÿ
A: å¯ä»¥ä½¿ç”¨æ­£åˆ™åŒ–ã€Dropoutã€æ•°æ®å¢å¼ºç­‰æ–¹æ³•ã€‚åŒæ—¶ç¡®ä¿éªŒè¯é›†çš„ä½¿ç”¨æ­£ç¡®ï¼Œé€‚æ—¶ä½¿ç”¨æ—©åœã€‚
