---
title: "é«˜æ€§èƒ½è®¡ç®—"
slug: "high-performance-computing"
sequence: 5
description: "AIå¼€å‘ä¸­çš„é«˜æ€§èƒ½è®¡ç®—æŠ€æœ¯ï¼ŒåŒ…æ‹¬NumPyä¼˜åŒ–ã€å¹¶è¡Œè®¡ç®—ã€GPUåŠ é€Ÿç­‰å…³é”®æŠ€èƒ½"
is_published: true
estimated_minutes: 90
language: "zh-CN"
---

# é«˜æ€§èƒ½è®¡ç®—

## è¯¾ç¨‹ä»‹ç»
æœ¬æ¨¡å—èšç„¦AIå¼€å‘ä¸­çš„é«˜æ€§èƒ½è®¡ç®—æŠ€æœ¯ï¼Œæ•™ä½ å¦‚ä½•ä¼˜åŒ–ä»£ç æ€§èƒ½ï¼Œå……åˆ†åˆ©ç”¨ç¡¬ä»¶èµ„æºã€‚é€šè¿‡å®é™…æ¡ˆä¾‹ï¼ŒæŒæ¡ä»CPUåˆ°GPUçš„å„ç§åŠ é€ŸæŠ€æœ¯ã€‚

## å­¦ä¹ ç›®æ ‡
å®Œæˆæœ¬æ¨¡å—å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ä½¿ç”¨NumPyè¿›è¡Œé«˜æ•ˆè®¡ç®—
- å®ç°å¹¶è¡Œå’Œåˆ†å¸ƒå¼è®¡ç®—
- ä½¿ç”¨GPUåŠ é€Ÿæ·±åº¦å­¦ä¹ 
- ä¼˜åŒ–å¤§è§„æ¨¡æ•°æ®å¤„ç†æ€§èƒ½

## 1. NumPyä¼˜åŒ–æŠ€å·§

### 1.1 å‘é‡åŒ–è¿ç®—
```python
# âš¡ï¸ å®æˆ˜æ¡ˆä¾‹ï¼šå›¾åƒå¤„ç†ä¼˜åŒ–
import numpy as np
import time

def process_image_loop(image):
    """ä½¿ç”¨å¾ªç¯å¤„ç†å›¾åƒï¼ˆæ…¢ï¼‰"""
    height, width = image.shape
    result = np.zeros_like(image)
    
    for i in range(1, height-1):
        for j in range(1, width-1):
            # 3x3å·ç§¯æ ¸
            result[i, j] = np.sum(image[i-1:i+2, j-1:j+2]) / 9
    
    return result

def process_image_vectorized(image):
    """ä½¿ç”¨å‘é‡åŒ–å¤„ç†å›¾åƒï¼ˆå¿«ï¼‰"""
    # ä½¿ç”¨å·ç§¯æ“ä½œ
    kernel = np.ones((3, 3)) / 9
    return np.correlate(image, kernel, mode='same')

# æ€§èƒ½å¯¹æ¯”
image = np.random.rand(1000, 1000)

start = time.time()
result_loop = process_image_loop(image)
print(f"Loop time: {time.time() - start:.2f}s")

start = time.time()
result_vectorized = process_image_vectorized(image)
print(f"Vectorized time: {time.time() - start:.2f}s")
```

### 1.2 å†…å­˜ä¼˜åŒ–
```python
# ğŸ’¾ å®æˆ˜æ¡ˆä¾‹ï¼šå†…å­˜ä¼˜åŒ–
def optimize_array_memory():
    """å†…å­˜ä½¿ç”¨ä¼˜åŒ–æŠ€å·§"""
    # 1. ä½¿ç”¨é€‚å½“çš„æ•°æ®ç±»å‹
    x = np.zeros(1000000, dtype=np.float32)  # è€Œä¸æ˜¯float64
    
    # 2. ä½¿ç”¨è§†å›¾è€Œä¸æ˜¯å¤åˆ¶
    y = x.view()  # è€Œä¸æ˜¯x.copy()
    
    # 3. ä½¿ç”¨å†…å­˜æ˜ å°„
    data = np.memmap('large_array.npy', dtype=np.float32, mode='w+', shape=(1000000,))
    
    # 4. ä½¿ç”¨ç”Ÿæˆå™¨å¤„ç†å¤§æ•°æ®
    def process_chunks(array, chunk_size=1000):
        for i in range(0, len(array), chunk_size):
            yield array[i:i + chunk_size]
```

## 2. å¹¶è¡Œè®¡ç®—åŸºç¡€

### 2.1 å¤šè¿›ç¨‹å¤„ç†
```python
# ğŸ”„ å®æˆ˜æ¡ˆä¾‹ï¼šå¹¶è¡Œæ•°æ®å¤„ç†
from multiprocessing import Pool
import numpy as np

def process_chunk(chunk):
    """å¤„ç†æ•°æ®å—çš„å‡½æ•°"""
    # ç¤ºä¾‹ï¼šè®¡ç®—æ¯ä¸ªæ•°å­—çš„å¹³æ–¹æ ¹å’Œå¹³æ–¹çš„å’Œ
    return np.sum(np.sqrt(chunk) + np.square(chunk))

def parallel_processing(data, num_processes=4):
    """å¹¶è¡Œå¤„ç†å¤§æ•°æ®é›†"""
    # å°†æ•°æ®åˆ†å‰²æˆå—
    chunks = np.array_split(data, num_processes)
    
    # åˆ›å»ºè¿›ç¨‹æ± 
    with Pool(num_processes) as pool:
        # å¹¶è¡Œå¤„ç†æ¯ä¸ªå—
        results = pool.map(process_chunk, chunks)
    
    # åˆå¹¶ç»“æœ
    return sum(results)

# ä½¿ç”¨ç¤ºä¾‹
data = np.random.rand(1000000)
result = parallel_processing(data)
```

### 2.2 å¼‚æ­¥å¤„ç†
```python
# âš¡ï¸ å®æˆ˜æ¡ˆä¾‹ï¼šå¼‚æ­¥æ•°æ®åŠ è½½
import asyncio
import aiohttp
import aiofiles

async def download_file(url, filename):
    """å¼‚æ­¥ä¸‹è½½æ–‡ä»¶"""
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            if response.status == 200:
                async with aiofiles.open(filename, mode='wb') as f:
                    await f.write(await response.read())
                return filename
            return None

async def process_file(filename):
    """å¼‚æ­¥å¤„ç†æ–‡ä»¶"""
    async with aiofiles.open(filename, mode='r') as f:
        content = await f.read()
        # å¤„ç†æ–‡ä»¶å†…å®¹
        return len(content)

async def main():
    """ä¸»å¼‚æ­¥å‡½æ•°"""
    # ä¸‹è½½ä»»åŠ¡
    download_tasks = [
        download_file(url, f"file_{i}.txt")
        for i, url in enumerate(urls)
    ]
    # å¹¶å‘ä¸‹è½½
    files = await asyncio.gather(*download_tasks)
    
    # å¤„ç†ä»»åŠ¡
    process_tasks = [
        process_file(f) for f in files if f is not None
    ]
    # å¹¶å‘å¤„ç†
    results = await asyncio.gather(*process_tasks)
    return results
```

## 3. GPUåŠ é€Ÿå…¥é—¨

### 3.1 CUDAåŸºç¡€
```python
# ğŸš€ å®æˆ˜æ¡ˆä¾‹ï¼šGPUåŠ é€Ÿè®¡ç®—
import torch

def cuda_acceleration_demo():
    """GPUåŠ é€Ÿç¤ºä¾‹"""
    # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    
    # åˆ›å»ºå¤§çŸ©é˜µ
    size = 10000
    a = torch.randn(size, size)
    b = torch.randn(size, size)
    
    # CPUè®¡ç®—
    start = time.time()
    c_cpu = torch.mm(a, b)
    cpu_time = time.time() - start
    print(f"CPU time: {cpu_time:.2f}s")
    
    # GPUè®¡ç®—
    if device.type == 'cuda':
        a = a.to(device)
        b = b.to(device)
        
        start = time.time()
        c_gpu = torch.mm(a, b)
        gpu_time = time.time() - start
        print(f"GPU time: {gpu_time:.2f}s")
        print(f"Speedup: {cpu_time/gpu_time:.1f}x")
```

### 3.2 CuPyä½¿ç”¨
```python
# ğŸ“Š å®æˆ˜æ¡ˆä¾‹ï¼šCuPyæ•°æ®å¤„ç†
import cupy as cp

def cupy_processing_demo():
    """CuPyå¤„ç†ç¤ºä¾‹"""
    # åˆ›å»ºGPUæ•°ç»„
    x_gpu = cp.random.random((1000, 1000))
    y_gpu = cp.random.random((1000, 1000))
    
    # GPUä¸Šè¿›è¡Œè®¡ç®—
    start = time.time()
    # çŸ©é˜µä¹˜æ³•
    z_gpu = cp.dot(x_gpu, y_gpu)
    # FFTå˜æ¢
    fft_gpu = cp.fft.fft2(z_gpu)
    # å·ç§¯æ“ä½œ
    kernel = cp.random.random((3, 3))
    conv_gpu = cp.correlate2d(z_gpu, kernel, mode='same')
    
    gpu_time = time.time() - start
    print(f"GPU processing time: {gpu_time:.2f}s")
    
    # å°†ç»“æœè½¬å›CPU
    z_cpu = cp.asnumpy(z_gpu)
    return z_cpu
```

## å®æˆ˜é¡¹ç›®ï¼šå›¾åƒå¤„ç†åŠ é€Ÿå™¨

### é¡¹ç›®æè¿°
å®ç°ä¸€ä¸ªé«˜æ€§èƒ½çš„å›¾åƒå¤„ç†ç³»ç»Ÿï¼Œç»“åˆCPUå’ŒGPUåŠ é€ŸæŠ€æœ¯ã€‚

### é¡¹ç›®ä»£ç æ¡†æ¶
```python
class ImageProcessor:
    def __init__(self, use_gpu=True):
        self.device = 'gpu' if use_gpu and torch.cuda.is_available() else 'cpu'
        self.model = self.load_model()
    
    def load_model(self):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        model = torch.hub.load('pytorch/vision:v0.10.0', 
                             'resnet18', pretrained=True)
        if self.device == 'gpu':
            model = model.cuda()
        return model
    
    def process_batch(self, images):
        """æ‰¹é‡å¤„ç†å›¾åƒ"""
        # è½¬æ¢ä¸ºå¼ é‡
        batch = torch.stack([self.preprocess(img) for img in images])
        if self.device == 'gpu':
            batch = batch.cuda()
        
        # ä½¿ç”¨æ¨¡å‹å¤„ç†
        with torch.no_grad():
            output = self.model(batch)
        
        # åå¤„ç†
        results = self.postprocess(output)
        return results
    
    def parallel_process(self, image_list, batch_size=32, num_workers=4):
        """å¹¶è¡Œå¤„ç†å¤§é‡å›¾åƒ"""
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataset = ImageDataset(image_list)
        loader = DataLoader(dataset, batch_size=batch_size, 
                          num_workers=num_workers)
        
        results = []
        for batch in loader:
            batch_results = self.process_batch(batch)
            results.extend(batch_results)
        
        return results
```

## ç»ƒä¹ ä¸ä½œä¸š
1. å®ç°çŸ©é˜µè¿ç®—çš„GPUåŠ é€Ÿ
2. åˆ›å»ºå¹¶è¡Œæ•°æ®å¤„ç†pipeline
3. ä¼˜åŒ–å¤§è§„æ¨¡è®¡ç®—æ€§èƒ½

## æ‰©å±•é˜…è¯»
- [NumPyæ€§èƒ½ä¼˜åŒ–æŒ‡å—](https://numpy.org/doc/stable/user/basics.html)
- [CUDAç¼–ç¨‹æŒ‡å—](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)
- [å¹¶è¡Œè®¡ç®—å®è·µ](https://python-parallel-programming-cookbook.readthedocs.io/)

## å°æµ‹éªŒ
1. å‘é‡åŒ–è®¡ç®—çš„ä¼˜åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ
2. å¦‚ä½•é€‰æ‹©åˆé€‚çš„å¹¶è¡Œç­–ç•¥ï¼Ÿ
3. GPUåŠ é€Ÿçš„é€‚ç”¨åœºæ™¯æœ‰å“ªäº›ï¼Ÿ

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- åˆ†å¸ƒå¼è®¡ç®—
- æ·±åº¦å­¦ä¹ ä¼˜åŒ–
- æ¨¡å‹éƒ¨ç½²åŠ é€Ÿ

## å¸¸è§é—®é¢˜è§£ç­”
Q: CPUå’ŒGPUå¦‚ä½•é€‰æ‹©ï¼Ÿ
A: æ ¹æ®ä»»åŠ¡ç‰¹ç‚¹é€‰æ‹©ï¼šCPUé€‚åˆé€»è¾‘å¤æ‚çš„ä¸²è¡Œä»»åŠ¡ï¼ŒGPUé€‚åˆå¤§è§„æ¨¡å¹¶è¡Œçš„æ•°å€¼è®¡ç®—ã€‚

Q: å¦‚ä½•é¿å…å†…å­˜æº¢å‡ºï¼Ÿ
A: ä½¿ç”¨ç”Ÿæˆå™¨ã€å†…å­˜æ˜ å°„ã€åˆ†æ‰¹å¤„ç†ç­‰æŠ€æœ¯ï¼Œé¿å…ä¸€æ¬¡åŠ è½½è¿‡å¤šæ•°æ®ã€‚
