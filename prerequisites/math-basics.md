---
title: "æ•°å­¦åŸºç¡€å…¥é—¨"
slug: "math-basics"
sequence: 2
description: "AIå¼€å‘æ‰€éœ€çš„æ ¸å¿ƒæ•°å­¦æ¦‚å¿µï¼ŒåŒ…æ‹¬çº¿æ€§ä»£æ•°ã€å¾®ç§¯åˆ†ã€æ¦‚ç‡ç»Ÿè®¡å’Œä¿¡æ¯è®ºåŸºç¡€"
is_published: true
estimated_minutes: 90
language: "zh-CN"
---

# æ•°å­¦åŸºç¡€å…¥é—¨

## è¯¾ç¨‹ä»‹ç»
æœ¬æ¨¡å—èšç„¦AIå¼€å‘ä¸­æœ€å¸¸ç”¨çš„æ•°å­¦æ¦‚å¿µå’Œå·¥å…·ã€‚é€šè¿‡å®é™…çš„AIåº”ç”¨æ¡ˆä¾‹ï¼Œå¸®åŠ©ä½ å»ºç«‹ç›´è§‚çš„æ•°å­¦è®¤è¯†ï¼Œä¸ºåç»­çš„æ·±åº¦å­¦ä¹ å’Œå¤§æ¨¡å‹å¼€å‘æ‰“ä¸‹åšå®åŸºç¡€ã€‚

## å­¦ä¹ ç›®æ ‡
å®Œæˆæœ¬æ¨¡å—å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ç†è§£AIä¸­çš„æ ¸å¿ƒæ•°å­¦æ¦‚å¿µ
- ä½¿ç”¨Pythonå®ç°æ•°å­¦è¿ç®—
- æŒæ¡æ•°å­¦å·¥å…·åœ¨AIä¸­çš„åº”ç”¨
- å…·å¤‡åŸºæœ¬çš„æ•°å­¦ç›´è§‰

## 1. çº¿æ€§ä»£æ•°åŸºç¡€

### 1.1 å‘é‡è¿ç®—
```python
# ğŸ”¢ å®æˆ˜æ¡ˆä¾‹ï¼šè¯å‘é‡è¿ç®—
import numpy as np

# åˆ›å»ºè¯å‘é‡
king = np.array([0.0, 0.7, 0.3, 0.2])
man = np.array([0.1, 0.4, 0.8, 0.3])
woman = np.array([0.1, 0.4, 0.2, 0.8])

# å‘é‡è¿ç®—ï¼šking - man + woman â‰ˆ queen
queen = king - man + woman

# è®¡ç®—ç›¸ä¼¼åº¦
def cosine_similarity(v1, v2):
    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))

print(f"ç›¸ä¼¼åº¦: {cosine_similarity(queen, np.array([0.0, 0.7, 0.2, 0.7])):.2f}")
```

### 1.2 çŸ©é˜µè¿ç®—
```python
# ğŸ“Š å®æˆ˜æ¡ˆä¾‹ï¼šå›¾åƒè½¬æ¢
def rotate_image(image, angle):
    """ä½¿ç”¨çŸ©é˜µæ—‹è½¬å›¾åƒ"""
    # åˆ›å»ºæ—‹è½¬çŸ©é˜µ
    theta = np.radians(angle)
    rotation_matrix = np.array([
        [np.cos(theta), -np.sin(theta)],
        [np.sin(theta), np.cos(theta)]
    ])
    
    # åº”ç”¨æ—‹è½¬
    return np.dot(image, rotation_matrix)
```

## 2. å¾®ç§¯åˆ†è¦ç‚¹

### 2.1 æ¢¯åº¦ä¸‹é™
```python
# ğŸ“‰ å®æˆ˜æ¡ˆä¾‹ï¼šç®€å•ç¥ç»ç½‘ç»œè®­ç»ƒ
def gradient_descent(X, y, learning_rate=0.01, epochs=100):
    """ä½¿ç”¨æ¢¯åº¦ä¸‹é™ä¼˜åŒ–çº¿æ€§æ¨¡å‹"""
    w = np.random.randn(X.shape[1])
    
    for epoch in range(epochs):
        # å‰å‘ä¼ æ’­
        predictions = np.dot(X, w)
        
        # è®¡ç®—æ¢¯åº¦
        gradient = np.dot(X.T, (predictions - y)) / len(y)
        
        # æ›´æ–°æƒé‡
        w -= learning_rate * gradient
        
        # è®¡ç®—æŸå¤±
        loss = np.mean((predictions - y) ** 2)
        if epoch % 10 == 0:
            print(f"Epoch {epoch}, Loss: {loss:.4f}")
    
    return w
```

### 2.2 é“¾å¼æ³•åˆ™
```python
# ğŸ”— å®æˆ˜æ¡ˆä¾‹ï¼šåå‘ä¼ æ’­
class SimpleNeuron:
    def __init__(self):
        self.w = np.random.randn()
        self.b = np.random.randn()
    
    def forward(self, x):
        self.x = x
        self.y = self.w * x + self.b
        return self.y
    
    def backward(self, grad_y):
        # ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ¢¯åº¦
        grad_w = grad_y * self.x
        grad_b = grad_y
        grad_x = grad_y * self.w
        return grad_x, grad_w, grad_b
```

## 3. æ¦‚ç‡ç»Ÿè®¡åŸºç¡€

### 3.1 æ¦‚ç‡åˆ†å¸ƒ
```python
# ğŸ“Š å®æˆ˜æ¡ˆä¾‹ï¼šç”Ÿæˆå¯¹æŠ—ç½‘ç»œä¸­çš„åˆ†å¸ƒ
import torch.nn as nn

class Generator(nn.Module):
    def __init__(self):
        super().__init__()
        self.model = nn.Sequential(
            # ä»æ­£æ€åˆ†å¸ƒç”Ÿæˆæ½œåœ¨å‘é‡
            nn.Linear(100, 256),
            nn.ReLU(),
            nn.Linear(256, 784),
            nn.Sigmoid()
        )
    
    def forward(self, z):
        # zæ˜¯ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·çš„å™ªå£°
        return self.model(z)
```

### 3.2 æœŸæœ›ä¸æ–¹å·®
```python
# ğŸ“ˆ å®æˆ˜æ¡ˆä¾‹ï¼šBatch Normalization
def batch_norm(x, eps=1e-5):
    """æ‰‹åŠ¨å®ç°æ‰¹é‡å½’ä¸€åŒ–"""
    # è®¡ç®—å‡å€¼
    mean = np.mean(x, axis=0)
    # è®¡ç®—æ–¹å·®
    var = np.var(x, axis=0)
    # å½’ä¸€åŒ–
    x_norm = (x - mean) / np.sqrt(var + eps)
    return x_norm
```

## 4. ä¿¡æ¯è®ºåŸºç¡€

### 4.1 ç†µä¸äº’ä¿¡æ¯
```python
# ğŸ” å®æˆ˜æ¡ˆä¾‹ï¼šç‰¹å¾é€‰æ‹©
from scipy.stats import entropy

def mutual_information(X, y):
    """è®¡ç®—ç‰¹å¾ä¸æ ‡ç­¾ä¹‹é—´çš„äº’ä¿¡æ¯"""
    # è®¡ç®—è”åˆæ¦‚ç‡åˆ†å¸ƒ
    joint_dist = np.histogram2d(X, y)[0]
    
    # è®¡ç®—è¾¹ç¼˜åˆ†å¸ƒ
    p_x = np.sum(joint_dist, axis=1)
    p_y = np.sum(joint_dist, axis=0)
    
    # è®¡ç®—äº’ä¿¡æ¯
    mi = np.sum(joint_dist * np.log(joint_dist / np.outer(p_x, p_y)))
    return mi
```

### 4.2 äº¤å‰ç†µ
```python
# ğŸ’¡ å®æˆ˜æ¡ˆä¾‹ï¼šåˆ†ç±»æ¨¡å‹æŸå¤±å‡½æ•°
def cross_entropy_loss(y_true, y_pred):
    """è®¡ç®—äº¤å‰ç†µæŸå¤±"""
    epsilon = 1e-15
    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
    return -np.sum(y_true * np.log(y_pred)) / len(y_true)
```

## å®æˆ˜é¡¹ç›®ï¼šå›¾åƒåˆ†ç±»å™¨

### é¡¹ç›®æè¿°
æ„å»ºä¸€ä¸ªç®€å•çš„å›¾åƒåˆ†ç±»å™¨ï¼Œç»¼åˆè¿ç”¨æœ¬æ¨¡å—å­¦ä¹ çš„æ•°å­¦æ¦‚å¿µã€‚

### é¡¹ç›®ä»£ç æ¡†æ¶
```python
class SimpleClassifier:
    def __init__(self, input_dim, num_classes):
        self.W = np.random.randn(input_dim, num_classes) * 0.01
        self.b = np.zeros(num_classes)
    
    def forward(self, X):
        # çº¿æ€§å˜æ¢
        scores = np.dot(X, self.W) + self.b
        # Softmaxæ¿€æ´»
        exp_scores = np.exp(scores)
        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
        return probs
    
    def train(self, X, y, learning_rate=1e-3, epochs=100):
        for epoch in range(epochs):
            # å‰å‘ä¼ æ’­
            probs = self.forward(X)
            
            # è®¡ç®—æ¢¯åº¦
            dW = np.dot(X.T, (probs - y)) / len(y)
            db = np.sum(probs - y, axis=0) / len(y)
            
            # æ›´æ–°å‚æ•°
            self.W -= learning_rate * dW
            self.b -= learning_rate * db
            
            # è®¡ç®—æŸå¤±
            loss = cross_entropy_loss(y, probs)
            if epoch % 10 == 0:
                print(f"Epoch {epoch}, Loss: {loss:.4f}")
```

## ç»ƒä¹ ä¸ä½œä¸š
1. å®ç°PCAé™ç»´ç®—æ³•
2. ç¼–å†™Mini-batchæ¢¯åº¦ä¸‹é™
3. è®¡ç®—ä¸åŒæ¿€æ´»å‡½æ•°çš„æ¢¯åº¦

## æ‰©å±•é˜…è¯»
- [çº¿æ€§ä»£æ•°åŠå…¶åº”ç”¨](https://book-url)
- [æ·±åº¦å­¦ä¹ ä¸­çš„æ•°å­¦](https://book-url)
- [ä¿¡æ¯è®ºåŸºç¡€](https://book-url)

## å°æµ‹éªŒ
1. ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–å¤„ç†ï¼Ÿ
2. æ¢¯åº¦ä¸‹é™çš„åŸç†æ˜¯ä»€ä¹ˆï¼Ÿ
3. äº¤å‰ç†µåœ¨æœºå™¨å­¦ä¹ ä¸­çš„ä½œç”¨ï¼Ÿ

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- æœºå™¨å­¦ä¹ ç®—æ³•
- æ·±åº¦å­¦ä¹ åŸºç¡€
- ä¼˜åŒ–æŠ€æœ¯

## å¸¸è§é—®é¢˜è§£ç­”
Q: ä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ è¿™äº›æ•°å­¦çŸ¥è¯†ï¼Ÿ
A: è¿™äº›æ•°å­¦æ¦‚å¿µæ˜¯ç†è§£å’Œä¼˜åŒ–AIæ¨¡å‹çš„åŸºç¡€ï¼Œèƒ½å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£æ¨¡å‹è¡Œä¸ºå’Œè°ƒä¼˜è¿‡ç¨‹ã€‚

Q: å¦‚ä½•æé«˜æ•°å­¦ç›´è§‰ï¼Ÿ
A: å¤šåšå®è·µç»ƒä¹ ï¼Œå°†æ•°å­¦æ¦‚å¿µä¸å®é™…çš„AIåº”ç”¨ç»“åˆèµ·æ¥ï¼Œé€šè¿‡å¯è§†åŒ–å’Œå®éªŒåŠ æ·±ç†è§£ã€‚

## 1. çº¿æ€§ä»£æ•°åŸºç¡€

### 1.1 å‘é‡åŸºç¡€
```python
import numpy as np

# å‘é‡çš„åˆ›å»ºä¸åŸºæœ¬è¿ç®—
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])

# å‘é‡åŠ å‡æ³•
v_sum = v1 + v2
v_diff = v1 - v2

# ç‚¹ç§¯
dot_product = np.dot(v1, v2)

# L1å’ŒL2èŒƒæ•°
l1_norm = np.sum(np.abs(v1))        # L1èŒƒæ•°
l2_norm = np.sqrt(np.sum(v1**2))    # L2èŒƒæ•°

# å‘é‡çš„åº”ç”¨ç¤ºä¾‹ï¼šæ–‡æœ¬å‘é‡åŒ–
from sklearn.feature_extraction.text import TfidfVectorizer

texts = [
    "æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£",
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é›†",
    "ç¥ç»ç½‘ç»œæ˜¯æ·±åº¦å­¦ä¹ çš„åŸºç¡€"
]

vectorizer = TfidfVectorizer()
text_vectors = vectorizer.fit_transform(texts)
```

### 1.2 çŸ©é˜µè¿ç®—
```python
# çŸ©é˜µçš„åˆ›å»ºä¸åŸºæœ¬è¿ç®—
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])

# çŸ©é˜µåŠ å‡æ³•
C = A + B
D = A - B

# çŸ©é˜µä¹˜æ³•
E = np.dot(A, B)  # æˆ– A @ B

# çŸ©é˜µè½¬ç½®
A_T = A.T

# ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
eigenvalues, eigenvectors = np.linalg.eig(A)

# çŸ©é˜µåˆ†è§£
U, S, V = np.linalg.svd(A)  # SVDåˆ†è§£
```

## 2. å¾®ç§¯åˆ†åŸºç¡€

### 2.1 å¯¼æ•°ä¸æ¢¯åº¦
```python
# ä½¿ç”¨numpyå®ç°æ•°å€¼å¯¼æ•°
def numerical_derivative(f, x, h=1e-7):
    return (f(x + h) - f(x)) / h

# ç®€å•çš„æ¢¯åº¦ä¸‹é™ç¤ºä¾‹
def gradient_descent(f, df, x0, learning_rate=0.01, n_iterations=100):
    x = x0
    history = [x]
    
    for _ in range(n_iterations):
        gradient = df(x)
        x = x - learning_rate * gradient
        history.append(x)
    
    return x, history

# ç¤ºä¾‹ï¼šæœ€å°åŒ–å‡½æ•° f(x) = x^2
def f(x): return x**2
def df(x): return 2*x

minimum, path = gradient_descent(f, df, x0=2.0)
```

### 2.2 åå¯¼æ•°ä¸é“¾å¼æ³•åˆ™
```python
# ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­ç¤ºä¾‹
class SimpleNeuron:
    def __init__(self):
        self.w = np.random.randn()
        self.b = np.random.randn()
    
    def forward(self, x):
        return x * self.w + self.b
    
    def backward(self, x, grad_output):
        # é“¾å¼æ³•åˆ™
        grad_w = x * grad_output
        grad_b = grad_output
        grad_x = self.w * grad_output
        return grad_w, grad_b, grad_x
```

## 3. æ¦‚ç‡ç»Ÿè®¡åŸºç¡€

### 3.1 æ¦‚ç‡åˆ†å¸ƒ
```python
import scipy.stats as stats

# æ­£æ€åˆ†å¸ƒ
mu, sigma = 0, 1
x = np.linspace(-3, 3, 100)
pdf = stats.norm.pdf(x, mu, sigma)

# äºŒé¡¹åˆ†å¸ƒ
n, p = 10, 0.5
k = np.arange(0, n+1)
pmf = stats.binom.pmf(k, n, p)

# æ³Šæ¾åˆ†å¸ƒ
lambda_ = 2
k = np.arange(0, 10)
pmf_poisson = stats.poisson.pmf(k, lambda_)
```

### 3.2 ç»Ÿè®¡æ¨æ–­
```python
# å‡è®¾æ£€éªŒç¤ºä¾‹
from scipy import stats

# ç”Ÿæˆä¸¤ç»„æ•°æ®
group1 = np.random.normal(0, 1, 1000)
group2 = np.random.normal(0.5, 1, 1000)

# tæ£€éªŒ
t_stat, p_value = stats.ttest_ind(group1, group2)

# ç½®ä¿¡åŒºé—´
confidence_interval = stats.t.interval(0.95, len(group1)-1,
                                     loc=np.mean(group1),
                                     scale=stats.sem(group1))
```

## 4. ä¿¡æ¯è®ºåŸºç¡€

### 4.1 ç†µä¸äº’ä¿¡æ¯
```python
from scipy.stats import entropy

# è®¡ç®—ç†µ
def calculate_entropy(p):
    return entropy(p)

# ç¤ºä¾‹ï¼šè®¡ç®—äºŒè¿›åˆ¶åºåˆ—çš„ç†µ
p = np.array([0.3, 0.7])  # æ¦‚ç‡åˆ†å¸ƒ
H = calculate_entropy(p)

# è®¡ç®—KLæ•£åº¦
def kl_divergence(p, q):
    return np.sum(p * np.log(p/q))

# è®¡ç®—äº’ä¿¡æ¯
def mutual_information(joint_prob, marginal_x, marginal_y):
    return kl_divergence(joint_prob, 
                        np.outer(marginal_x, marginal_y))
```

### 4.2 äº¤å‰ç†µä¸æŸå¤±å‡½æ•°
```python
def cross_entropy(y_true, y_pred):
    """è®¡ç®—äº¤å‰ç†µæŸå¤±"""
    return -np.sum(y_true * np.log(y_pred + 1e-15))

# ç®€å•åˆ†ç±»å™¨ç¤ºä¾‹
class SimpleClassifier:
    def __init__(self, input_dim, num_classes):
        self.W = np.random.randn(input_dim, num_classes) * 0.01
        self.b = np.zeros(num_classes)
    
    def forward(self, X):
        scores = np.dot(X, self.W) + self.b
        exp_scores = np.exp(scores)
        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
        return probs
    
    def loss(self, X, y):
        probs = self.forward(X)
        N = X.shape[0]
        loss = -np.sum(np.log(probs[np.arange(N), y])) / N
        return loss
```

## å¸¸è§é—®é¢˜è§£ç­”

Q: ä¸ºä»€ä¹ˆéœ€è¦å­¦ä¹ è¿™äº›æ•°å­¦åŸºç¡€ï¼Ÿ
A: è¿™äº›æ•°å­¦æ¦‚å¿µæ˜¯ç†è§£å’Œå®ç°AIç®—æ³•çš„åŸºç¡€ã€‚ä¾‹å¦‚ï¼Œçº¿æ€§ä»£æ•°ç”¨äºæ•°æ®è¡¨ç¤ºå’Œè¿ç®—ï¼Œå¾®ç§¯åˆ†ç”¨äºä¼˜åŒ–ç®—æ³•ï¼Œæ¦‚ç‡ç»Ÿè®¡ç”¨äºæ¨¡å‹è¯„ä¼°å’Œé¢„æµ‹ã€‚

Q: å¦‚ä½•æé«˜æ•°å­¦ç›´è§‰ï¼Ÿ
A: å¤šåŠ¨æ‰‹å®è·µï¼Œå°†æ•°å­¦æ¦‚å¿µä¸å®é™…çš„AIåº”ç”¨ç»“åˆèµ·æ¥ã€‚é€šè¿‡å¯è§†åŒ–å’Œç¼–ç¨‹å®ç°æ¥åŠ æ·±ç†è§£ã€‚

Q: éœ€è¦æŒæ¡åˆ°ä»€ä¹ˆç¨‹åº¦ï¼Ÿ
A: é‡ç‚¹æ˜¯ç†è§£æ ¸å¿ƒæ¦‚å¿µå’Œå®ƒä»¬åœ¨AIä¸­çš„åº”ç”¨ã€‚ä¸éœ€è¦æŒæ¡æ‰€æœ‰æ•°å­¦è¯æ˜ï¼Œä½†è¦èƒ½å¤Ÿè¿ç”¨è¿™äº›å·¥å…·è§£å†³å®é™…é—®é¢˜ã€‚
