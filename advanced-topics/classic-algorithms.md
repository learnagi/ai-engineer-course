---
title: "ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•"
slug: "classic-ml"
sequence: 8
description: "å¸¸ç”¨æœºå™¨å­¦ä¹ ç®—æ³•çš„åŸç†å’Œå®ç°ï¼ŒåŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ ç®—æ³•åŠå…¶å®è·µåº”ç”¨"
is_published: true
estimated_minutes: 120
language: "zh-CN"
---

# ç»å…¸æœºå™¨å­¦ä¹ ç®—æ³•

## è¯¾ç¨‹ä»‹ç»
æœ¬æ¨¡å—æ·±å…¥è®²è§£å¸¸ç”¨æœºå™¨å­¦ä¹ ç®—æ³•çš„åŸç†å’Œå®ç°ï¼Œé€šè¿‡å®é™…æ¡ˆä¾‹å¸®åŠ©ä½ æŒæ¡å„ç±»ç®—æ³•çš„ç‰¹ç‚¹å’Œåº”ç”¨åœºæ™¯ã€‚

## å­¦ä¹ ç›®æ ‡
å®Œæˆæœ¬æ¨¡å—å­¦ä¹ åï¼Œä½ å°†èƒ½å¤Ÿï¼š
- ç†è§£ä¸»æµæœºå™¨å­¦ä¹ ç®—æ³•çš„åŸç†
- æŒæ¡ç®—æ³•é€‰æ‹©çš„æ–¹æ³•
- å®ç°åŸºæœ¬çš„æœºå™¨å­¦ä¹ ç®—æ³•
- åº”ç”¨ç®—æ³•è§£å†³å®é™…é—®é¢˜

## 1. ç›‘ç£å­¦ä¹ ç®—æ³•

### 1.1 çº¿æ€§æ¨¡å‹
```python
# ğŸ“ˆ å®æˆ˜æ¡ˆä¾‹ï¼šçº¿æ€§æ¨¡å‹å®ç°
import numpy as np
from sklearn.base import BaseEstimator, RegressorMixin

class LinearRegressionFromScratch(BaseEstimator, RegressorMixin):
    """ä»é›¶å®ç°çº¿æ€§å›å½’"""
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.history = []
    
    def fit(self, X, y):
        # åˆå§‹åŒ–å‚æ•°
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # æ¢¯åº¦ä¸‹é™
        for i in range(self.n_iterations):
            # é¢„æµ‹
            y_pred = self._forward(X)
            
            # è®¡ç®—æ¢¯åº¦
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            # æ›´æ–°å‚æ•°
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db
            
            # è®°å½•æŸå¤±
            loss = np.mean((y_pred - y) ** 2)
            self.history.append(loss)
        
        return self
    
    def _forward(self, X):
        return np.dot(X, self.weights) + self.bias
    
    def predict(self, X):
        return self._forward(X)

# ä½¿ç”¨ç¤ºä¾‹
def linear_model_demo():
    """çº¿æ€§æ¨¡å‹ç¤ºä¾‹"""
    from sklearn.datasets import make_regression
    
    # ç”Ÿæˆæ•°æ®
    X, y = make_regression(n_samples=100, n_features=1, noise=10)
    
    # è®­ç»ƒæ¨¡å‹
    model = LinearRegressionFromScratch()
    model.fit(X, y)
    
    # å¯è§†åŒ–ç»“æœ
    plt.figure(figsize=(10, 5))
    
    # ç»˜åˆ¶æ•°æ®ç‚¹
    plt.subplot(1, 2, 1)
    plt.scatter(X, y)
    plt.plot(X, model.predict(X), 'r')
    plt.title('æ‹Ÿåˆç»“æœ')
    
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.subplot(1, 2, 2)
    plt.plot(model.history)
    plt.title('æŸå¤±æ›²çº¿')
    
    return plt.gcf()
```

### 1.2 å†³ç­–æ ‘ä¸é›†æˆå­¦ä¹ 
```python
# ğŸŒ³ å®æˆ˜æ¡ˆä¾‹ï¼šå†³ç­–æ ‘ä¸éšæœºæ£®æ—
class DecisionTreeFromScratch:
    """ä»é›¶å®ç°å†³ç­–æ ‘"""
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.tree = None
    
    def fit(self, X, y):
        self.n_classes = len(np.unique(y))
        self.tree = self._grow_tree(X, y)
        return self
    
    def _grow_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape
        n_labels = len(np.unique(y))
        
        # åœæ­¢æ¡ä»¶
        if (self.max_depth is not None and depth >= self.max_depth) or \
           n_labels == 1:
            return {'value': np.bincount(y).argmax()}
        
        # å¯»æ‰¾æœ€ä½³åˆ†å‰²
        best_gain = -1
        best_split = None
        
        for feature in range(n_features):
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                gain = self._information_gain(y, X[:, feature], threshold)
                if gain > best_gain:
                    best_gain = gain
                    best_split = (feature, threshold)
        
        if best_split is None:
            return {'value': np.bincount(y).argmax()}
        
        # åˆ†å‰²æ•°æ®
        feature, threshold = best_split
        left_mask = X[:, feature] <= threshold
        right_mask = ~left_mask
        
        # é€’å½’æ„å»ºå­æ ‘
        left_tree = self._grow_tree(X[left_mask], y[left_mask], depth + 1)
        right_tree = self._grow_tree(X[right_mask], y[right_mask], depth + 1)
        
        return {
            'feature': feature,
            'threshold': threshold,
            'left': left_tree,
            'right': right_tree
        }
    
    def _information_gain(self, y, X_column, threshold):
        """è®¡ç®—ä¿¡æ¯å¢ç›Š"""
        parent_entropy = self._entropy(y)
        
        # åˆ†å‰²æ•°æ®
        left_mask = X_column <= threshold
        right_mask = ~left_mask
        
        if len(y[left_mask]) == 0 or len(y[right_mask]) == 0:
            return 0
        
        # è®¡ç®—å­èŠ‚ç‚¹ç†µ
        n = len(y)
        n_l, n_r = len(y[left_mask]), len(y[right_mask])
        e_l, e_r = self._entropy(y[left_mask]), self._entropy(y[right_mask])
        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r
        
        return parent_entropy - child_entropy
    
    def _entropy(self, y):
        """è®¡ç®—ç†µ"""
        proportions = np.bincount(y) / len(y)
        return -np.sum([p * np.log2(p) for p in proportions if p > 0])
    
    def predict(self, X):
        return np.array([self._traverse_tree(x, self.tree) for x in X])
    
    def _traverse_tree(self, x, node):
        """éå†å†³ç­–æ ‘è¿›è¡Œé¢„æµ‹"""
        if 'value' in node:
            return node['value']
        
        if x[node['feature']] <= node['threshold']:
            return self._traverse_tree(x, node['left'])
        return self._traverse_tree(x, node['right'])

# éšæœºæ£®æ—å®ç°
class RandomForestFromScratch:
    """ä»é›¶å®ç°éšæœºæ£®æ—"""
    def __init__(self, n_trees=10, max_depth=None):
        self.n_trees = n_trees
        self.max_depth = max_depth
        self.trees = []
    
    def fit(self, X, y):
        self.trees = []
        for _ in range(self.n_trees):
            # éšæœºæŠ½æ ·ï¼ˆBootstrapï¼‰
            indices = np.random.choice(len(X), len(X), replace=True)
            sample_X = X[indices]
            sample_y = y[indices]
            
            # è®­ç»ƒå†³ç­–æ ‘
            tree = DecisionTreeFromScratch(max_depth=self.max_depth)
            tree.fit(sample_X, sample_y)
            self.trees.append(tree)
        return self
    
    def predict(self, X):
        # æ”¶é›†æ‰€æœ‰æ ‘çš„é¢„æµ‹
        predictions = np.array([tree.predict(X) for tree in self.trees])
        # å¤šæ•°æŠ•ç¥¨
        return np.array([np.bincount(pred).argmax() 
                        for pred in predictions.T])
```

## 2. æ— ç›‘ç£å­¦ä¹ ç®—æ³•

### 2.1 K-meansèšç±»
```python
# ğŸ¯ å®æˆ˜æ¡ˆä¾‹ï¼šK-meansèšç±»å®ç°
class KMeansFromScratch:
    """ä»é›¶å®ç°K-meansèšç±»"""
    def __init__(self, n_clusters=3, max_iters=100):
        self.n_clusters = n_clusters
        self.max_iters = max_iters
        self.centroids = None
    
    def fit(self, X):
        # éšæœºåˆå§‹åŒ–è´¨å¿ƒ
        idx = np.random.choice(len(X), self.n_clusters, replace=False)
        self.centroids = X[idx]
        
        for _ in range(self.max_iters):
            # åˆ†é…æ ·æœ¬åˆ°æœ€è¿‘çš„è´¨å¿ƒ
            distances = self._calculate_distances(X)
            labels = np.argmin(distances, axis=1)
            
            # æ›´æ–°è´¨å¿ƒ
            new_centroids = np.array([X[labels == k].mean(axis=0) 
                                    for k in range(self.n_clusters)])
            
            # æ£€æŸ¥æ”¶æ•›
            if np.all(self.centroids == new_centroids):
                break
                
            self.centroids = new_centroids
        
        return self
    
    def _calculate_distances(self, X):
        """è®¡ç®—æ ·æœ¬åˆ°æ‰€æœ‰è´¨å¿ƒçš„è·ç¦»"""
        distances = np.zeros((len(X), self.n_clusters))
        for k, centroid in enumerate(self.centroids):
            distances[:, k] = np.sqrt(((X - centroid) ** 2).sum(axis=1))
        return distances
    
    def predict(self, X):
        distances = self._calculate_distances(X)
        return np.argmin(distances, axis=1)

# èšç±»å¯è§†åŒ–
def visualize_clusters(X, labels):
    """å¯è§†åŒ–èšç±»ç»“æœ"""
    plt.figure(figsize=(10, 6))
    scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')
    plt.colorbar(scatter)
    plt.title('èšç±»ç»“æœ')
    return plt.gcf()
```

### 2.2 ä¸»æˆåˆ†åˆ†æ(PCA)
```python
# ğŸ“Š å®æˆ˜æ¡ˆä¾‹ï¼šPCAå®ç°
class PCAFromScratch:
    """ä»é›¶å®ç°PCA"""
    def __init__(self, n_components=2):
        self.n_components = n_components
        self.components = None
        self.mean = None
    
    def fit(self, X):
        # ä¸­å¿ƒåŒ–
        self.mean = np.mean(X, axis=0)
        X_centered = X - self.mean
        
        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
        cov_matrix = np.cov(X_centered.T)
        
        # ç‰¹å¾å€¼åˆ†è§£
        eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
        
        # é€‰æ‹©å‰n_componentsä¸ªç‰¹å¾å‘é‡
        idx = np.argsort(eigenvalues)[::-1]
        self.components = eigenvectors[:, idx[:self.n_components]]
        
        # è®¡ç®—è§£é‡Šæ–¹å·®æ¯”
        self.explained_variance_ratio_ = eigenvalues[idx[:self.n_components]] / \
                                       np.sum(eigenvalues)
        
        return self
    
    def transform(self, X):
        X_centered = X - self.mean
        return np.dot(X_centered, self.components)
    
    def inverse_transform(self, X_transformed):
        return np.dot(X_transformed, self.components.T) + self.mean

# PCAå¯è§†åŒ–
def visualize_pca(X, y):
    """å¯è§†åŒ–PCAç»“æœ"""
    pca = PCAFromScratch(n_components=2)
    X_pca = pca.fit_transform(X)
    
    plt.figure(figsize=(12, 5))
    
    # ç»˜åˆ¶é™ç»´ç»“æœ
    plt.subplot(1, 2, 1)
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
    plt.colorbar(scatter)
    plt.title('PCAé™ç»´ç»“æœ')
    
    # ç»˜åˆ¶è§£é‡Šæ–¹å·®æ¯”
    plt.subplot(1, 2, 2)
    plt.bar(range(len(pca.explained_variance_ratio_)), 
            pca.explained_variance_ratio_)
    plt.title('è§£é‡Šæ–¹å·®æ¯”')
    
    return plt.gcf()
```

## å®æˆ˜é¡¹ç›®ï¼šå®¢æˆ·åˆ†ç±»ç³»ç»Ÿ

### é¡¹ç›®æè¿°
æ„å»ºä¸€ä¸ªå®¢æˆ·åˆ†ç±»ç³»ç»Ÿï¼Œç»“åˆç›‘ç£å’Œæ— ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œå®ç°å®¢æˆ·ç¾¤ä½“çš„åˆ†æå’Œé¢„æµ‹ã€‚

### é¡¹ç›®ä»£ç æ¡†æ¶
```python
class CustomerClassificationSystem:
    def __init__(self):
        self.clustering_model = None
        self.classification_model = None
        self.pca_model = None
    
    def preprocess_data(self, data):
        """æ•°æ®é¢„å¤„ç†"""
        # å¤„ç†ç¼ºå¤±å€¼
        imputer = SimpleImputer(strategy='mean')
        data_imputed = imputer.fit_transform(data)
        
        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        data_scaled = scaler.fit_transform(data_imputed)
        
        return data_scaled
    
    def cluster_customers(self, X, n_clusters=5):
        """å®¢æˆ·èšç±»"""
        # é™ç»´
        self.pca_model = PCAFromScratch(n_components=2)
        X_pca = self.pca_model.fit_transform(X)
        
        # èšç±»
        self.clustering_model = KMeansFromScratch(n_clusters=n_clusters)
        clusters = self.clustering_model.fit_predict(X_pca)
        
        return clusters, X_pca
    
    def train_classifier(self, X, y):
        """è®­ç»ƒåˆ†ç±»å™¨"""
        # ä½¿ç”¨éšæœºæ£®æ—åˆ†ç±»å™¨
        self.classification_model = RandomForestFromScratch(
            n_trees=100, max_depth=10
        )
        self.classification_model.fit(X, y)
    
    def analyze_customer_groups(self, X, clusters):
        """åˆ†æå®¢æˆ·ç¾¤ä½“ç‰¹å¾"""
        analysis = {}
        for cluster in range(max(clusters) + 1):
            cluster_data = X[clusters == cluster]
            analysis[f'Cluster_{cluster}'] = {
                'size': len(cluster_data),
                'mean': np.mean(cluster_data, axis=0),
                'std': np.std(cluster_data, axis=0)
            }
        return analysis
    
    def visualize_results(self, X_pca, clusters, feature_names=None):
        """å¯è§†åŒ–ç»“æœ"""
        plt.figure(figsize=(15, 5))
        
        # èšç±»ç»“æœ
        plt.subplot(1, 3, 1)
        scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], 
                            c=clusters, cmap='viridis')
        plt.colorbar(scatter)
        plt.title('å®¢æˆ·ç¾¤ä½“åˆ†å¸ƒ')
        
        # ç‰¹å¾é‡è¦æ€§
        if self.classification_model and feature_names is not None:
            importances = np.mean([tree.feature_importances_ 
                                 for tree in self.classification_model.trees], 
                                axis=0)
            plt.subplot(1, 3, 2)
            plt.bar(range(len(importances)), importances)
            plt.xticks(range(len(importances)), feature_names, rotation=45)
            plt.title('ç‰¹å¾é‡è¦æ€§')
        
        # ç¾¤ä½“å¤§å°å¯¹æ¯”
        plt.subplot(1, 3, 3)
        cluster_sizes = np.bincount(clusters)
        plt.pie(cluster_sizes, labels=[f'Group {i}' 
                for i in range(len(cluster_sizes))])
        plt.title('ç¾¤ä½“è§„æ¨¡å¯¹æ¯”')
        
        return plt.gcf()
```

## ç»ƒä¹ ä¸ä½œä¸š
1. å®ç°é€»è¾‘å›å½’ç®—æ³•
2. ä¼˜åŒ–å†³ç­–æ ‘çš„åˆ†è£‚ç­–ç•¥
3. å®ç°DBSCANèšç±»ç®—æ³•

## æ‰©å±•é˜…è¯»
- [ç»Ÿè®¡å­¦ä¹ æ–¹æ³•](https://book.douban.com/subject/33437381/)
- [æœºå™¨å­¦ä¹ å®æˆ˜](https://book.douban.com/subject/24703171/)
- [scikit-learnç®—æ³•æ–‡æ¡£](https://scikit-learn.org/stable/supervised_learning.html)

## å°æµ‹éªŒ
1. å†³ç­–æ ‘çš„ä¼˜ç¼ºç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
2. K-meansç®—æ³•çš„å±€é™æ€§æœ‰å“ªäº›ï¼Ÿ
3. å¦‚ä½•é€‰æ‹©åˆé€‚çš„èšç±»ç®—æ³•ï¼Ÿ

## ä¸‹ä¸€æ­¥å­¦ä¹ 
- æ·±åº¦å­¦ä¹ åŸºç¡€
- é›†æˆå­¦ä¹ è¿›é˜¶
- æ¨¡å‹éƒ¨ç½²å®è·µ

## å¸¸è§é—®é¢˜è§£ç­”
Q: å¦‚ä½•å¤„ç†å†³ç­–æ ‘çš„è¿‡æ‹Ÿåˆé—®é¢˜ï¼Ÿ
A: å¯ä»¥é€šè¿‡è®¾ç½®æœ€å¤§æ·±åº¦ã€æœ€å°æ ·æœ¬æ•°ã€å‰ªæç­‰æ–¹æ³•æ¥æ§åˆ¶å†³ç­–æ ‘çš„å¤æ‚åº¦ã€‚

Q: ä»€ä¹ˆæƒ…å†µä¸‹åº”è¯¥ä½¿ç”¨æ— ç›‘ç£å­¦ä¹ ï¼Ÿ
A: å½“æ•°æ®æ²¡æœ‰æ ‡ç­¾ã€éœ€è¦å‘ç°æ•°æ®å†…åœ¨ç»“æ„ã€æˆ–éœ€è¦é™ç»´æ—¶ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨æ— ç›‘ç£å­¦ä¹ æ–¹æ³•ã€‚
